{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdb9f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xie_k\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "# torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import splitfolders \n",
    "import cv2\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import math \n",
    "import time \n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3705f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTKDataset(Dataset):\n",
    "    '''\n",
    "        Inputs:\n",
    "            dataFrame : Pandas dataFrame\n",
    "            transform : The transform to apply to the dataset\n",
    "    '''\n",
    "    def __init__(self, dataFrame, transform=None):\n",
    "        self.transform = transform\n",
    "        \n",
    "        data_holder = dataFrame.pixels.apply(lambda x: np.array(x.split(\" \"),dtype=float))\n",
    "        arr = np.stack(data_holder)\n",
    "        arr = arr / 255.0\n",
    "        arr = arr.astype('float32')\n",
    "        arr = np.array([arr, arr, arr])\n",
    "        arr = arr.reshape(arr.shape[1], 48, 48, 3)\n",
    "        # reshape into 48x48x1\n",
    "        self.data = arr\n",
    "        \n",
    "#         self.age_label = np.array(dataFrame.age[:])      \n",
    "        self.gender_label = np.array(dataFrame.gender[:])\n",
    "#         self.eth_label = np.array(dataFrame.ethnicity[:])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        data = self.transform(data)\n",
    "        \n",
    "#         labels = torch.tensor((self.age_label[index], self.gender_label[index], self.eth_label[index]))\n",
    "\n",
    "#         labels = torch.tensor((self.age_label[index]))\n",
    "        labels = torch.tensor((self.gender_label[index]))\n",
    "   \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50fd5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54f82ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c59bd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: torch.Size([128, 3, 48, 48])\n",
      "Shape of lables: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "dataFrame = pd.read_csv('age_gender.gz', compression='gzip')\n",
    "\n",
    "# Construct age bins\n",
    "# age_bins = [0,10,15,20,25,30,40,50,60,120]\n",
    "# age_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# dataFrame['bins'] = pd.cut(dataFrame.age, bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Split into training and testing\n",
    "train_dataFrame, validation_test_dataFrame = train_test_split(dataFrame, test_size=0.2)\n",
    "validation_dataFrame, test_dataFrame = train_test_split(validation_test_dataFrame, test_size=0.5)\n",
    "\n",
    "# get the number of unique classes for each group\n",
    "class_nums = {'age_num':len(dataFrame['age'].unique()), 'gen_num':len(dataFrame['gender'].unique())}\n",
    "\n",
    "# Define train and test transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Construct the custom pytorch datasets\n",
    "train_set = UTKDataset(train_dataFrame, transform=train_transform)\n",
    "validation_set = UTKDataset(test_dataFrame, transform=validation_transform)\n",
    "test_set = UTKDataset(test_dataFrame, transform=test_transform)\n",
    "\n",
    "# Load the datasets into dataloaders\n",
    "trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "validationloader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Sanity Check\n",
    "for data, lables in trainloader:\n",
    "    print(f'Shape of training data: {data.shape}')\n",
    "    print(f'Shape of lables: {lables.shape}')\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed55fe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 18964, 'val': 2371}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders = {'train': trainloader, 'val': validationloader}\n",
    "dataset_sizes = {'train': len(train_set), 'val': len(validation_set)}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ae2da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25088\n",
      "Epoch [1/10], Step [1/149], Loss: 0.6832\n",
      "Epoch [1/10], Step [2/149], Loss: 0.6926\n",
      "Epoch [1/10], Step [3/149], Loss: 0.6997\n",
      "Epoch [1/10], Step [4/149], Loss: 0.6695\n",
      "Epoch [1/10], Step [5/149], Loss: 0.7067\n",
      "Epoch [1/10], Step [6/149], Loss: 0.7106\n",
      "Epoch [1/10], Step [7/149], Loss: 0.7221\n",
      "Epoch [1/10], Step [8/149], Loss: 0.6992\n",
      "Epoch [1/10], Step [9/149], Loss: 0.6959\n",
      "Epoch [1/10], Step [10/149], Loss: 0.6935\n",
      "Epoch [1/10], Step [11/149], Loss: 0.6919\n",
      "Epoch [1/10], Step [12/149], Loss: 0.6807\n",
      "Epoch [1/10], Step [13/149], Loss: 0.6922\n",
      "Epoch [1/10], Step [14/149], Loss: 0.6976\n",
      "Epoch [1/10], Step [15/149], Loss: 0.7027\n",
      "Epoch [1/10], Step [16/149], Loss: 0.6968\n",
      "Epoch [1/10], Step [17/149], Loss: 0.6958\n",
      "Epoch [1/10], Step [18/149], Loss: 0.6996\n",
      "Epoch [1/10], Step [19/149], Loss: 0.6966\n",
      "Epoch [1/10], Step [20/149], Loss: 0.6886\n",
      "Epoch [1/10], Step [21/149], Loss: 0.6912\n",
      "Epoch [1/10], Step [22/149], Loss: 0.6897\n",
      "Epoch [1/10], Step [23/149], Loss: 0.6968\n",
      "Epoch [1/10], Step [24/149], Loss: 0.6971\n",
      "Epoch [1/10], Step [25/149], Loss: 0.6872\n",
      "Epoch [1/10], Step [26/149], Loss: 0.6947\n",
      "Epoch [1/10], Step [27/149], Loss: 0.6949\n",
      "Epoch [1/10], Step [28/149], Loss: 0.7007\n",
      "Epoch [1/10], Step [29/149], Loss: 0.6889\n",
      "Epoch [1/10], Step [30/149], Loss: 0.6918\n",
      "Epoch [1/10], Step [31/149], Loss: 0.6929\n",
      "Epoch [1/10], Step [32/149], Loss: 0.6928\n",
      "Epoch [1/10], Step [33/149], Loss: 0.6935\n",
      "Epoch [1/10], Step [34/149], Loss: 0.6881\n",
      "Epoch [1/10], Step [35/149], Loss: 0.6995\n",
      "Epoch [1/10], Step [36/149], Loss: 0.6964\n",
      "Epoch [1/10], Step [37/149], Loss: 0.6855\n",
      "Epoch [1/10], Step [38/149], Loss: 0.6914\n",
      "Epoch [1/10], Step [39/149], Loss: 0.6942\n",
      "Epoch [1/10], Step [40/149], Loss: 0.6966\n",
      "Epoch [1/10], Step [41/149], Loss: 0.6995\n",
      "Epoch [1/10], Step [42/149], Loss: 0.6907\n",
      "Epoch [1/10], Step [43/149], Loss: 0.6998\n",
      "Epoch [1/10], Step [44/149], Loss: 0.6899\n",
      "Epoch [1/10], Step [45/149], Loss: 0.6946\n",
      "Epoch [1/10], Step [46/149], Loss: 0.6928\n",
      "Epoch [1/10], Step [47/149], Loss: 0.6918\n",
      "Epoch [1/10], Step [48/149], Loss: 0.6951\n",
      "Epoch [1/10], Step [49/149], Loss: 0.6893\n",
      "Epoch [1/10], Step [50/149], Loss: 0.6876\n",
      "Epoch [1/10], Step [51/149], Loss: 0.6904\n",
      "Epoch [1/10], Step [52/149], Loss: 0.6898\n",
      "Epoch [1/10], Step [53/149], Loss: 0.6963\n",
      "Epoch [1/10], Step [54/149], Loss: 0.6893\n",
      "Epoch [1/10], Step [55/149], Loss: 0.6959\n",
      "Epoch [1/10], Step [56/149], Loss: 0.6973\n",
      "Epoch [1/10], Step [57/149], Loss: 0.6896\n",
      "Epoch [1/10], Step [58/149], Loss: 0.6994\n",
      "Epoch [1/10], Step [59/149], Loss: 0.6921\n",
      "Epoch [1/10], Step [60/149], Loss: 0.6866\n",
      "Epoch [1/10], Step [61/149], Loss: 0.6950\n",
      "Epoch [1/10], Step [62/149], Loss: 0.7016\n",
      "Epoch [1/10], Step [63/149], Loss: 0.6808\n",
      "Epoch [1/10], Step [64/149], Loss: 0.6920\n",
      "Epoch [1/10], Step [65/149], Loss: 0.6914\n",
      "Epoch [1/10], Step [66/149], Loss: 0.7002\n",
      "Epoch [1/10], Step [67/149], Loss: 0.6910\n",
      "Epoch [1/10], Step [68/149], Loss: 0.6834\n",
      "Epoch [1/10], Step [69/149], Loss: 0.6978\n",
      "Epoch [1/10], Step [70/149], Loss: 0.6821\n",
      "Epoch [1/10], Step [71/149], Loss: 0.6944\n",
      "Epoch [1/10], Step [72/149], Loss: 0.6837\n",
      "Epoch [1/10], Step [73/149], Loss: 0.6945\n",
      "Epoch [1/10], Step [74/149], Loss: 0.6890\n",
      "Epoch [1/10], Step [75/149], Loss: 0.6905\n",
      "Epoch [1/10], Step [76/149], Loss: 0.6982\n",
      "Epoch [1/10], Step [77/149], Loss: 0.7015\n",
      "Epoch [1/10], Step [78/149], Loss: 0.6973\n",
      "Epoch [1/10], Step [79/149], Loss: 0.6906\n",
      "Epoch [1/10], Step [80/149], Loss: 0.6880\n",
      "Epoch [1/10], Step [81/149], Loss: 0.6918\n",
      "Epoch [1/10], Step [82/149], Loss: 0.7054\n",
      "Epoch [1/10], Step [83/149], Loss: 0.6914\n",
      "Epoch [1/10], Step [84/149], Loss: 0.7020\n",
      "Epoch [1/10], Step [85/149], Loss: 0.6931\n",
      "Epoch [1/10], Step [86/149], Loss: 0.6856\n",
      "Epoch [1/10], Step [87/149], Loss: 0.6924\n",
      "Epoch [1/10], Step [88/149], Loss: 0.6942\n",
      "Epoch [1/10], Step [89/149], Loss: 0.6971\n",
      "Epoch [1/10], Step [90/149], Loss: 0.6938\n",
      "Epoch [1/10], Step [91/149], Loss: 0.6939\n",
      "Epoch [1/10], Step [92/149], Loss: 0.6972\n",
      "Epoch [1/10], Step [93/149], Loss: 0.6928\n",
      "Epoch [1/10], Step [94/149], Loss: 0.6963\n",
      "Epoch [1/10], Step [95/149], Loss: 0.6937\n",
      "Epoch [1/10], Step [96/149], Loss: 0.6953\n",
      "Epoch [1/10], Step [97/149], Loss: 0.6956\n",
      "Epoch [1/10], Step [98/149], Loss: 0.6905\n",
      "Epoch [1/10], Step [99/149], Loss: 0.6958\n",
      "Epoch [1/10], Step [100/149], Loss: 0.6883\n",
      "Epoch [1/10], Step [101/149], Loss: 0.6933\n",
      "Epoch [1/10], Step [102/149], Loss: 0.7023\n",
      "Epoch [1/10], Step [103/149], Loss: 0.7028\n",
      "Epoch [1/10], Step [104/149], Loss: 0.6916\n",
      "Epoch [1/10], Step [105/149], Loss: 0.6971\n",
      "Epoch [1/10], Step [106/149], Loss: 0.6971\n",
      "Epoch [1/10], Step [107/149], Loss: 0.6940\n",
      "Epoch [1/10], Step [108/149], Loss: 0.6952\n",
      "Epoch [1/10], Step [109/149], Loss: 0.6910\n",
      "Epoch [1/10], Step [110/149], Loss: 0.6859\n",
      "Epoch [1/10], Step [111/149], Loss: 0.7004\n",
      "Epoch [1/10], Step [112/149], Loss: 0.6888\n",
      "Epoch [1/10], Step [113/149], Loss: 0.6951\n",
      "Epoch [1/10], Step [114/149], Loss: 0.6982\n",
      "Epoch [1/10], Step [115/149], Loss: 0.6946\n",
      "Epoch [1/10], Step [116/149], Loss: 0.6909\n",
      "Epoch [1/10], Step [117/149], Loss: 0.6949\n",
      "Epoch [1/10], Step [118/149], Loss: 0.6931\n",
      "Epoch [1/10], Step [119/149], Loss: 0.6914\n",
      "Epoch [1/10], Step [120/149], Loss: 0.6951\n",
      "Epoch [1/10], Step [121/149], Loss: 0.6859\n",
      "Epoch [1/10], Step [122/149], Loss: 0.6916\n",
      "Epoch [1/10], Step [123/149], Loss: 0.6942\n",
      "Epoch [1/10], Step [124/149], Loss: 0.6913\n",
      "Epoch [1/10], Step [125/149], Loss: 0.6970\n",
      "Epoch [1/10], Step [126/149], Loss: 0.6942\n",
      "Epoch [1/10], Step [127/149], Loss: 0.6924\n",
      "Epoch [1/10], Step [128/149], Loss: 0.6896\n",
      "Epoch [1/10], Step [129/149], Loss: 0.6909\n",
      "Epoch [1/10], Step [130/149], Loss: 0.6934\n",
      "Epoch [1/10], Step [131/149], Loss: 0.6937\n",
      "Epoch [1/10], Step [132/149], Loss: 0.6928\n",
      "Epoch [1/10], Step [133/149], Loss: 0.6919\n",
      "Epoch [1/10], Step [134/149], Loss: 0.6958\n",
      "Epoch [1/10], Step [135/149], Loss: 0.6968\n",
      "Epoch [1/10], Step [136/149], Loss: 0.6952\n",
      "Epoch [1/10], Step [137/149], Loss: 0.6929\n",
      "Epoch [1/10], Step [138/149], Loss: 0.6941\n",
      "Epoch [1/10], Step [139/149], Loss: 0.6974\n",
      "Epoch [1/10], Step [140/149], Loss: 0.6918\n",
      "Epoch [1/10], Step [141/149], Loss: 0.6863\n",
      "Epoch [1/10], Step [142/149], Loss: 0.6947\n",
      "Epoch [1/10], Step [143/149], Loss: 0.6931\n",
      "Epoch [1/10], Step [144/149], Loss: 0.6929\n",
      "Epoch [1/10], Step [145/149], Loss: 0.6952\n",
      "Epoch [1/10], Step [146/149], Loss: 0.6932\n",
      "Epoch [1/10], Step [147/149], Loss: 0.6886\n",
      "Epoch [1/10], Step [148/149], Loss: 0.6938\n",
      "Epoch [1/10], Step [149/149], Loss: 0.6876\n",
      "Epoch [1/10] and the best match:\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0])\n",
      "tensor([[ 1.3992e-01, -1.1425e-03],\n",
      "        [ 1.9470e-01, -2.0810e-01],\n",
      "        [ 1.6602e-01, -1.1459e-01],\n",
      "        [ 3.9366e-02, -1.3830e-01],\n",
      "        [ 1.2865e-01, -1.2282e-01],\n",
      "        [ 1.7707e-01, -4.9612e-02],\n",
      "        [ 3.1025e-01, -3.3917e-01],\n",
      "        [ 1.4263e-01, -1.7399e-01],\n",
      "        [ 8.5564e-02,  2.5908e-02],\n",
      "        [ 2.5150e-01, -2.6228e-02],\n",
      "        [ 9.3844e-02, -2.8710e-02],\n",
      "        [ 1.0735e-01, -1.7812e-01],\n",
      "        [ 2.5198e-01, -8.5493e-02],\n",
      "        [ 3.4518e-02, -1.5737e-01],\n",
      "        [ 2.9578e-02, -8.0334e-02],\n",
      "        [ 1.9181e-01, -2.6982e-01],\n",
      "        [ 8.0793e-02, -3.8219e-02],\n",
      "        [ 2.0423e-01, -1.0553e-01],\n",
      "        [ 1.1473e-01, -8.9602e-02],\n",
      "        [ 2.2689e-01, -2.2986e-01],\n",
      "        [ 3.0142e-01, -1.9838e-01],\n",
      "        [ 1.1073e-01, -7.6657e-02],\n",
      "        [ 3.9091e-01, -3.8485e-01],\n",
      "        [ 1.9574e-01, -1.4797e-01],\n",
      "        [ 2.3892e-01, -1.0815e-01],\n",
      "        [ 9.4953e-02, -2.2782e-01],\n",
      "        [ 1.1627e-01, -2.4513e-01],\n",
      "        [ 1.8539e-01, -1.3333e-01],\n",
      "        [-7.8251e-03, -8.8708e-02],\n",
      "        [ 1.9466e-01, -2.4697e-01],\n",
      "        [ 2.1564e-01, -7.2672e-02],\n",
      "        [ 2.7853e-01, -3.2906e-01],\n",
      "        [ 9.7971e-02, -1.2576e-01],\n",
      "        [ 1.6594e-01, -1.3283e-01],\n",
      "        [ 2.1203e-01, -2.0815e-01],\n",
      "        [ 3.0931e-01, -1.0939e-01],\n",
      "        [ 1.9997e-01, -2.1664e-02],\n",
      "        [ 8.8876e-02, -4.6348e-02],\n",
      "        [ 2.6069e-03, -5.7096e-02],\n",
      "        [ 1.0707e-01, -7.1001e-02],\n",
      "        [ 1.3003e-01, -1.3237e-01],\n",
      "        [ 2.4988e-01, -1.4358e-01],\n",
      "        [ 3.2416e-01, -1.2133e-01],\n",
      "        [ 9.0564e-02,  4.3251e-02],\n",
      "        [ 2.0751e-01, -1.4104e-01],\n",
      "        [ 3.0361e-02, -2.0430e-02],\n",
      "        [ 1.1180e-01, -1.2910e-01],\n",
      "        [ 1.3001e-01, -1.1712e-02],\n",
      "        [ 4.9370e-02, -6.5772e-02],\n",
      "        [ 1.1446e-01, -1.1726e-01],\n",
      "        [ 3.6336e-01, -2.3983e-01],\n",
      "        [ 2.1208e-01, -2.6819e-02],\n",
      "        [ 2.1237e-01, -1.2794e-01],\n",
      "        [ 1.5140e-01,  2.5615e-02],\n",
      "        [ 1.2434e-01, -7.0952e-02],\n",
      "        [ 3.8839e-02, -6.0118e-02],\n",
      "        [ 2.2730e-01, -1.3046e-01],\n",
      "        [ 1.3371e-01, -1.9501e-01],\n",
      "        [ 1.8485e-01, -1.4234e-01],\n",
      "        [ 2.0601e-01, -6.6346e-02],\n",
      "        [ 7.4494e-02, -3.6429e-02],\n",
      "        [ 3.0900e-01, -1.5479e-01],\n",
      "        [ 1.3989e-01, -3.7985e-02],\n",
      "        [ 6.9468e-02, -1.0515e-01],\n",
      "        [ 2.2340e-01, -1.3835e-01],\n",
      "        [ 1.6788e-01, -1.0257e-01],\n",
      "        [ 2.6025e-01, -7.6618e-05],\n",
      "        [ 1.5668e-01, -8.6832e-02],\n",
      "        [ 1.1375e-01, -5.1567e-02],\n",
      "        [ 2.4741e-01,  3.3713e-02],\n",
      "        [ 1.8576e-01, -8.7525e-02],\n",
      "        [ 7.5943e-02, -2.5573e-03],\n",
      "        [ 5.8035e-02, -5.9612e-02],\n",
      "        [ 8.4013e-02, -3.2152e-02],\n",
      "        [ 1.4673e-01, -1.2053e-01],\n",
      "        [ 2.2955e-01, -1.5826e-01],\n",
      "        [ 1.7289e-01, -4.6220e-02],\n",
      "        [ 1.1753e-01, -1.7765e-02],\n",
      "        [ 1.3516e-01, -1.0114e-01],\n",
      "        [ 2.5950e-01, -1.3102e-01],\n",
      "        [ 1.2709e-01,  7.5088e-03],\n",
      "        [ 1.1240e-01, -1.3382e-01],\n",
      "        [ 2.3306e-01, -2.0513e-01],\n",
      "        [ 2.7694e-01, -9.1995e-02],\n",
      "        [ 6.3201e-02, -1.4204e-01],\n",
      "        [ 1.1223e-01, -1.5700e-01],\n",
      "        [ 1.5189e-01, -3.9036e-02],\n",
      "        [ 2.5048e-01, -6.4123e-02],\n",
      "        [ 7.1094e-02,  5.2552e-02],\n",
      "        [ 1.3723e-01, -2.4853e-01],\n",
      "        [-9.7625e-04,  1.1342e-02],\n",
      "        [ 8.6836e-02, -4.4433e-02],\n",
      "        [ 2.2849e-01, -1.2103e-01],\n",
      "        [ 1.5776e-01, -1.0658e-01],\n",
      "        [ 4.7215e-02, -3.1149e-02],\n",
      "        [ 1.0674e-01, -8.8348e-02],\n",
      "        [ 5.4480e-02, -1.4323e-01],\n",
      "        [ 2.4124e-01, -1.4464e-01],\n",
      "        [ 2.5610e-01, -1.6869e-01],\n",
      "        [ 7.1074e-02, -6.7328e-02],\n",
      "        [ 2.6216e-02, -1.3214e-02],\n",
      "        [ 1.0529e-01, -4.5347e-02],\n",
      "        [ 5.3817e-02, -1.0311e-01],\n",
      "        [ 1.6833e-01, -2.6124e-02],\n",
      "        [ 1.0558e-01,  4.2706e-02],\n",
      "        [ 2.2089e-01, -1.5199e-01],\n",
      "        [ 1.1882e-01, -2.5170e-01],\n",
      "        [ 1.9071e-01, -2.6434e-01],\n",
      "        [ 9.7441e-02, -6.5971e-02],\n",
      "        [ 9.1265e-02, -8.6986e-02],\n",
      "        [ 1.7831e-01, -1.2632e-01],\n",
      "        [ 8.3102e-02, -6.4108e-02],\n",
      "        [ 1.4094e-01, -4.4171e-02],\n",
      "        [ 7.9953e-02, -2.0417e-01],\n",
      "        [ 1.0094e-01, -6.2275e-02],\n",
      "        [ 1.8695e-01, -1.3462e-01],\n",
      "        [ 2.2202e-01, -2.5529e-01],\n",
      "        [ 2.7466e-02, -1.7786e-02],\n",
      "        [ 1.6247e-01, -5.6194e-02],\n",
      "        [ 1.6946e-01, -1.0202e-01],\n",
      "        [ 8.4677e-02, -3.2872e-02],\n",
      "        [ 7.6213e-02,  4.8926e-02],\n",
      "        [ 1.9464e-01, -1.9444e-01],\n",
      "        [ 2.1165e-01, -1.1300e-01],\n",
      "        [ 6.9756e-04, -1.8571e-02],\n",
      "        [ 1.1413e-01, -1.8292e-01],\n",
      "        [ 1.2195e-01, -3.4733e-02],\n",
      "        [ 1.2318e-01, -1.8465e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [1/149], Loss: 0.6919\n",
      "Epoch [2/10], Step [2/149], Loss: 0.6838\n",
      "Epoch [2/10], Step [3/149], Loss: 0.6919\n",
      "Epoch [2/10], Step [4/149], Loss: 0.6905\n",
      "Epoch [2/10], Step [5/149], Loss: 0.6981\n",
      "Epoch [2/10], Step [6/149], Loss: 0.6994\n",
      "Epoch [2/10], Step [7/149], Loss: 0.6920\n",
      "Epoch [2/10], Step [8/149], Loss: 0.7024\n",
      "Epoch [2/10], Step [9/149], Loss: 0.6942\n",
      "Epoch [2/10], Step [10/149], Loss: 0.6982\n",
      "Epoch [2/10], Step [11/149], Loss: 0.6953\n",
      "Epoch [2/10], Step [12/149], Loss: 0.6931\n",
      "Epoch [2/10], Step [13/149], Loss: 0.6892\n",
      "Epoch [2/10], Step [14/149], Loss: 0.6911\n",
      "Epoch [2/10], Step [15/149], Loss: 0.6894\n",
      "Epoch [2/10], Step [16/149], Loss: 0.6927\n",
      "Epoch [2/10], Step [17/149], Loss: 0.6926\n",
      "Epoch [2/10], Step [18/149], Loss: 0.6896\n",
      "Epoch [2/10], Step [19/149], Loss: 0.6906\n",
      "Epoch [2/10], Step [20/149], Loss: 0.6926\n",
      "Epoch [2/10], Step [21/149], Loss: 0.6952\n",
      "Epoch [2/10], Step [22/149], Loss: 0.6939\n",
      "Epoch [2/10], Step [23/149], Loss: 0.6931\n",
      "Epoch [2/10], Step [24/149], Loss: 0.6874\n",
      "Epoch [2/10], Step [25/149], Loss: 0.6908\n",
      "Epoch [2/10], Step [26/149], Loss: 0.6935\n",
      "Epoch [2/10], Step [27/149], Loss: 0.6925\n",
      "Epoch [2/10], Step [28/149], Loss: 0.6895\n",
      "Epoch [2/10], Step [29/149], Loss: 0.6912\n",
      "Epoch [2/10], Step [30/149], Loss: 0.6902\n",
      "Epoch [2/10], Step [31/149], Loss: 0.6906\n",
      "Epoch [2/10], Step [32/149], Loss: 0.6876\n",
      "Epoch [2/10], Step [33/149], Loss: 0.6896\n",
      "Epoch [2/10], Step [34/149], Loss: 0.6910\n",
      "Epoch [2/10], Step [35/149], Loss: 0.6920\n",
      "Epoch [2/10], Step [36/149], Loss: 0.6900\n",
      "Epoch [2/10], Step [37/149], Loss: 0.6969\n",
      "Epoch [2/10], Step [38/149], Loss: 0.6902\n",
      "Epoch [2/10], Step [39/149], Loss: 0.6889\n",
      "Epoch [2/10], Step [40/149], Loss: 0.6929\n",
      "Epoch [2/10], Step [41/149], Loss: 0.7006\n",
      "Epoch [2/10], Step [42/149], Loss: 0.6933\n",
      "Epoch [2/10], Step [43/149], Loss: 0.6949\n",
      "Epoch [2/10], Step [44/149], Loss: 0.6898\n",
      "Epoch [2/10], Step [45/149], Loss: 0.6861\n",
      "Epoch [2/10], Step [46/149], Loss: 0.6880\n",
      "Epoch [2/10], Step [47/149], Loss: 0.6906\n",
      "Epoch [2/10], Step [48/149], Loss: 0.6851\n",
      "Epoch [2/10], Step [49/149], Loss: 0.6921\n",
      "Epoch [2/10], Step [50/149], Loss: 0.6862\n",
      "Epoch [2/10], Step [51/149], Loss: 0.6853\n",
      "Epoch [2/10], Step [52/149], Loss: 0.6881\n",
      "Epoch [2/10], Step [53/149], Loss: 0.6822\n",
      "Epoch [2/10], Step [54/149], Loss: 0.6769\n",
      "Epoch [2/10], Step [55/149], Loss: 0.6832\n",
      "Epoch [2/10], Step [56/149], Loss: 0.6958\n",
      "Epoch [2/10], Step [57/149], Loss: 0.6924\n",
      "Epoch [2/10], Step [58/149], Loss: 0.6833\n",
      "Epoch [2/10], Step [59/149], Loss: 0.6923\n",
      "Epoch [2/10], Step [60/149], Loss: 0.7041\n",
      "Epoch [2/10], Step [61/149], Loss: 0.6832\n",
      "Epoch [2/10], Step [62/149], Loss: 0.6951\n",
      "Epoch [2/10], Step [63/149], Loss: 0.7030\n",
      "Epoch [2/10], Step [64/149], Loss: 0.6976\n",
      "Epoch [2/10], Step [65/149], Loss: 0.6837\n",
      "Epoch [2/10], Step [66/149], Loss: 0.6885\n",
      "Epoch [2/10], Step [67/149], Loss: 0.6954\n",
      "Epoch [2/10], Step [68/149], Loss: 0.6982\n",
      "Epoch [2/10], Step [69/149], Loss: 0.6893\n",
      "Epoch [2/10], Step [70/149], Loss: 0.6926\n",
      "Epoch [2/10], Step [71/149], Loss: 0.6799\n",
      "Epoch [2/10], Step [72/149], Loss: 0.6938\n",
      "Epoch [2/10], Step [73/149], Loss: 0.6861\n",
      "Epoch [2/10], Step [74/149], Loss: 0.6932\n",
      "Epoch [2/10], Step [75/149], Loss: 0.6939\n",
      "Epoch [2/10], Step [76/149], Loss: 0.6790\n",
      "Epoch [2/10], Step [77/149], Loss: 0.6895\n",
      "Epoch [2/10], Step [78/149], Loss: 0.6907\n",
      "Epoch [2/10], Step [79/149], Loss: 0.6917\n",
      "Epoch [2/10], Step [80/149], Loss: 0.6926\n",
      "Epoch [2/10], Step [81/149], Loss: 0.6871\n",
      "Epoch [2/10], Step [82/149], Loss: 0.6903\n",
      "Epoch [2/10], Step [83/149], Loss: 0.6901\n",
      "Epoch [2/10], Step [84/149], Loss: 0.6881\n",
      "Epoch [2/10], Step [85/149], Loss: 0.6888\n",
      "Epoch [2/10], Step [86/149], Loss: 0.7033\n",
      "Epoch [2/10], Step [87/149], Loss: 0.6860\n",
      "Epoch [2/10], Step [88/149], Loss: 0.6994\n",
      "Epoch [2/10], Step [89/149], Loss: 0.6893\n",
      "Epoch [2/10], Step [90/149], Loss: 0.6892\n",
      "Epoch [2/10], Step [91/149], Loss: 0.6893\n",
      "Epoch [2/10], Step [92/149], Loss: 0.6946\n",
      "Epoch [2/10], Step [93/149], Loss: 0.6875\n",
      "Epoch [2/10], Step [94/149], Loss: 0.6927\n",
      "Epoch [2/10], Step [95/149], Loss: 0.6932\n",
      "Epoch [2/10], Step [96/149], Loss: 0.6843\n",
      "Epoch [2/10], Step [97/149], Loss: 0.6904\n",
      "Epoch [2/10], Step [98/149], Loss: 0.6895\n",
      "Epoch [2/10], Step [99/149], Loss: 0.6860\n",
      "Epoch [2/10], Step [100/149], Loss: 0.6840\n",
      "Epoch [2/10], Step [101/149], Loss: 0.6892\n",
      "Epoch [2/10], Step [102/149], Loss: 0.6984\n",
      "Epoch [2/10], Step [103/149], Loss: 0.6908\n",
      "Epoch [2/10], Step [104/149], Loss: 0.6991\n",
      "Epoch [2/10], Step [105/149], Loss: 0.7024\n",
      "Epoch [2/10], Step [106/149], Loss: 0.6891\n",
      "Epoch [2/10], Step [107/149], Loss: 0.6962\n",
      "Epoch [2/10], Step [108/149], Loss: 0.6943\n",
      "Epoch [2/10], Step [109/149], Loss: 0.6906\n",
      "Epoch [2/10], Step [110/149], Loss: 0.6958\n",
      "Epoch [2/10], Step [111/149], Loss: 0.6854\n",
      "Epoch [2/10], Step [112/149], Loss: 0.6849\n",
      "Epoch [2/10], Step [113/149], Loss: 0.6864\n",
      "Epoch [2/10], Step [114/149], Loss: 0.6962\n",
      "Epoch [2/10], Step [115/149], Loss: 0.6851\n",
      "Epoch [2/10], Step [116/149], Loss: 0.6897\n",
      "Epoch [2/10], Step [117/149], Loss: 0.6939\n",
      "Epoch [2/10], Step [118/149], Loss: 0.6843\n",
      "Epoch [2/10], Step [119/149], Loss: 0.6924\n",
      "Epoch [2/10], Step [120/149], Loss: 0.6876\n",
      "Epoch [2/10], Step [121/149], Loss: 0.6846\n",
      "Epoch [2/10], Step [122/149], Loss: 0.6935\n",
      "Epoch [2/10], Step [123/149], Loss: 0.6939\n",
      "Epoch [2/10], Step [124/149], Loss: 0.6856\n",
      "Epoch [2/10], Step [125/149], Loss: 0.6924\n",
      "Epoch [2/10], Step [126/149], Loss: 0.6982\n",
      "Epoch [2/10], Step [127/149], Loss: 0.6820\n",
      "Epoch [2/10], Step [128/149], Loss: 0.6973\n",
      "Epoch [2/10], Step [129/149], Loss: 0.6984\n",
      "Epoch [2/10], Step [130/149], Loss: 0.6885\n",
      "Epoch [2/10], Step [131/149], Loss: 0.6838\n",
      "Epoch [2/10], Step [132/149], Loss: 0.6950\n",
      "Epoch [2/10], Step [133/149], Loss: 0.6876\n",
      "Epoch [2/10], Step [134/149], Loss: 0.6921\n",
      "Epoch [2/10], Step [135/149], Loss: 0.6916\n",
      "Epoch [2/10], Step [136/149], Loss: 0.7002\n",
      "Epoch [2/10], Step [137/149], Loss: 0.7038\n",
      "Epoch [2/10], Step [138/149], Loss: 0.6959\n",
      "Epoch [2/10], Step [139/149], Loss: 0.6889\n",
      "Epoch [2/10], Step [140/149], Loss: 0.6837\n",
      "Epoch [2/10], Step [141/149], Loss: 0.6932\n",
      "Epoch [2/10], Step [142/149], Loss: 0.7008\n",
      "Epoch [2/10], Step [143/149], Loss: 0.6872\n",
      "Epoch [2/10], Step [144/149], Loss: 0.6873\n",
      "Epoch [2/10], Step [145/149], Loss: 0.6915\n",
      "Epoch [2/10], Step [146/149], Loss: 0.6945\n",
      "Epoch [2/10], Step [147/149], Loss: 0.6940\n",
      "Epoch [2/10], Step [148/149], Loss: 0.6905\n",
      "Epoch [2/10], Step [149/149], Loss: 0.7057\n",
      "Epoch [2/10] and the best match:\n",
      "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0])\n",
      "tensor([[ 1.2883e-01,  3.7360e-02],\n",
      "        [ 7.7286e-02,  7.3625e-02],\n",
      "        [ 5.8848e-02,  4.8062e-02],\n",
      "        [ 8.9586e-02, -3.0895e-02],\n",
      "        [-8.5270e-04,  6.3858e-02],\n",
      "        [ 1.4821e-01, -7.5748e-02],\n",
      "        [ 9.1836e-02, -2.4464e-02],\n",
      "        [ 6.7722e-02, -9.4662e-03],\n",
      "        [ 1.1095e-01, -8.1569e-02],\n",
      "        [ 6.8279e-02,  4.6728e-03],\n",
      "        [ 9.0752e-02,  1.1247e-02],\n",
      "        [ 1.5988e-01, -2.0287e-02],\n",
      "        [ 1.0557e-01,  4.0704e-02],\n",
      "        [ 6.6013e-02,  2.6106e-02],\n",
      "        [ 9.9791e-02, -2.3068e-02],\n",
      "        [ 4.6383e-02,  3.4570e-02],\n",
      "        [ 6.6248e-02,  3.4460e-02],\n",
      "        [ 9.7617e-02, -1.0648e-01],\n",
      "        [ 1.0989e-01, -4.0063e-02],\n",
      "        [ 1.6951e-01, -1.1630e-02],\n",
      "        [ 1.1151e-01, -6.7261e-02],\n",
      "        [ 1.5394e-01,  4.3754e-03],\n",
      "        [ 2.3558e-02,  1.9014e-02],\n",
      "        [ 7.2252e-02, -3.0019e-02],\n",
      "        [ 1.4811e-01,  1.2170e-02],\n",
      "        [ 1.3109e-01, -1.7195e-02],\n",
      "        [ 7.1963e-02, -1.2825e-02],\n",
      "        [ 1.6543e-01, -7.9099e-02],\n",
      "        [ 6.6507e-02, -5.6974e-02],\n",
      "        [ 2.7205e-02,  1.4872e-02],\n",
      "        [ 7.7148e-02, -3.1931e-03],\n",
      "        [ 1.0044e-01, -6.4849e-03],\n",
      "        [ 4.2578e-02,  3.1896e-02],\n",
      "        [ 6.3055e-02,  4.0866e-03],\n",
      "        [ 8.4475e-02, -2.8674e-02],\n",
      "        [ 1.4375e-01, -5.3606e-02],\n",
      "        [ 8.8838e-02,  6.0671e-03],\n",
      "        [ 1.3520e-01, -7.3155e-02],\n",
      "        [ 1.5497e-01, -5.4758e-02],\n",
      "        [ 6.1847e-02,  3.9306e-02],\n",
      "        [ 1.1237e-01, -2.4472e-02],\n",
      "        [ 1.3668e-01, -1.9608e-02],\n",
      "        [ 7.0410e-02,  4.2225e-02],\n",
      "        [ 1.3581e-01, -1.6087e-02],\n",
      "        [ 1.2079e-01, -7.5353e-02],\n",
      "        [ 3.4101e-02,  2.6998e-02],\n",
      "        [ 3.7790e-02,  3.5160e-02],\n",
      "        [ 1.7604e-01, -5.6720e-02],\n",
      "        [ 4.7522e-02, -9.5255e-03],\n",
      "        [ 2.1233e-01, -9.6493e-02],\n",
      "        [ 1.1936e-01, -3.6346e-02],\n",
      "        [ 2.1460e-01, -4.6224e-02],\n",
      "        [ 2.3417e-01, -3.1617e-02],\n",
      "        [ 9.1975e-02, -4.0697e-03],\n",
      "        [ 1.2659e-01, -1.8544e-02],\n",
      "        [ 1.5236e-01,  9.1236e-05],\n",
      "        [ 1.1318e-01,  3.2716e-02],\n",
      "        [ 2.0937e-01, -1.2311e-01],\n",
      "        [ 1.9462e-01, -2.3068e-01],\n",
      "        [ 2.1171e-01, -7.0498e-02],\n",
      "        [ 1.3229e-01, -1.9281e-02],\n",
      "        [ 2.4161e-01, -3.0254e-02],\n",
      "        [ 1.0823e-01, -1.3689e-02],\n",
      "        [ 1.0771e-01, -1.9488e-03],\n",
      "        [ 1.2634e-01, -7.6616e-02],\n",
      "        [ 1.1259e-01, -8.5581e-02],\n",
      "        [ 9.9245e-02,  3.1379e-02],\n",
      "        [ 2.6674e-02,  4.2034e-02],\n",
      "        [ 6.4747e-02, -1.4739e-02],\n",
      "        [ 4.9294e-02,  1.3916e-02],\n",
      "        [ 3.5197e-02,  2.0374e-03],\n",
      "        [ 1.7214e-01, -1.0970e-02],\n",
      "        [ 6.2161e-02, -1.6513e-02],\n",
      "        [ 1.6666e-01, -1.6314e-02],\n",
      "        [ 1.8168e-01, -1.1596e-01],\n",
      "        [ 1.0717e-01,  8.7364e-03],\n",
      "        [ 7.6444e-02,  4.4279e-02],\n",
      "        [ 1.2407e-01, -6.8859e-02],\n",
      "        [ 6.5793e-02, -3.1713e-03],\n",
      "        [ 1.0281e-01, -2.5709e-02],\n",
      "        [ 1.3000e-01, -2.3237e-02],\n",
      "        [ 1.7770e-02, -1.1147e-02],\n",
      "        [ 8.5346e-02, -3.9581e-02],\n",
      "        [ 9.6575e-02, -1.4372e-02],\n",
      "        [ 8.0663e-02, -7.2469e-02],\n",
      "        [ 1.3673e-01, -7.8142e-02],\n",
      "        [ 1.0071e-01, -3.9525e-02],\n",
      "        [ 4.7042e-02, -1.7348e-03],\n",
      "        [ 6.1375e-02,  1.8214e-02],\n",
      "        [ 1.3830e-01, -2.3666e-02],\n",
      "        [ 1.7746e-01, -5.5594e-02],\n",
      "        [ 1.4842e-01, -5.7197e-02],\n",
      "        [ 1.1146e-01, -2.5213e-02],\n",
      "        [ 1.2144e-01,  6.1654e-02],\n",
      "        [ 1.4914e-01, -4.0985e-02],\n",
      "        [ 1.1163e-01, -5.1442e-02],\n",
      "        [ 2.0620e-02,  3.0677e-02],\n",
      "        [ 7.1569e-02, -2.4611e-02],\n",
      "        [ 8.2556e-02,  8.5409e-03],\n",
      "        [ 2.1285e-01, -1.3618e-01],\n",
      "        [ 1.0417e-01, -3.8750e-02],\n",
      "        [ 1.8899e-01, -1.0839e-02],\n",
      "        [ 5.7618e-02,  1.6482e-02],\n",
      "        [ 1.1403e-01, -5.1611e-02],\n",
      "        [ 5.4046e-02,  5.3825e-02],\n",
      "        [ 9.6187e-02, -1.5024e-02],\n",
      "        [ 5.5116e-02,  3.4772e-02],\n",
      "        [ 1.3255e-01,  4.8366e-02],\n",
      "        [ 1.3817e-01,  1.4068e-02],\n",
      "        [ 9.8835e-02, -5.5899e-02],\n",
      "        [ 1.0363e-01, -2.0773e-02],\n",
      "        [ 1.4912e-01, -2.8072e-02],\n",
      "        [ 7.1335e-02, -4.0491e-02],\n",
      "        [ 7.8428e-02, -2.7481e-02],\n",
      "        [ 4.2053e-02,  2.9766e-03],\n",
      "        [ 1.6064e-01, -4.0223e-02],\n",
      "        [ 6.5986e-02,  2.9557e-02],\n",
      "        [ 8.6928e-02, -7.1756e-02],\n",
      "        [ 1.2415e-01, -9.0067e-02],\n",
      "        [ 6.6193e-02, -1.0569e-01],\n",
      "        [ 6.8190e-02, -1.3828e-02],\n",
      "        [ 3.7118e-02,  6.9836e-02],\n",
      "        [ 6.8221e-02,  1.5235e-02],\n",
      "        [ 5.2629e-02,  2.4617e-02],\n",
      "        [ 9.0929e-02,  3.2292e-02],\n",
      "        [ 1.3522e-01, -8.6555e-02],\n",
      "        [ 8.5719e-02,  1.2617e-02],\n",
      "        [ 1.0526e-01, -1.6717e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [1/149], Loss: 0.6880\n",
      "Epoch [3/10], Step [2/149], Loss: 0.6895\n",
      "Epoch [3/10], Step [3/149], Loss: 0.6805\n",
      "Epoch [3/10], Step [4/149], Loss: 0.6862\n",
      "Epoch [3/10], Step [5/149], Loss: 0.6807\n",
      "Epoch [3/10], Step [6/149], Loss: 0.6896\n",
      "Epoch [3/10], Step [7/149], Loss: 0.6829\n",
      "Epoch [3/10], Step [8/149], Loss: 0.6921\n",
      "Epoch [3/10], Step [9/149], Loss: 0.6866\n",
      "Epoch [3/10], Step [10/149], Loss: 0.6895\n",
      "Epoch [3/10], Step [11/149], Loss: 0.6840\n",
      "Epoch [3/10], Step [12/149], Loss: 0.6835\n",
      "Epoch [3/10], Step [13/149], Loss: 0.6976\n",
      "Epoch [3/10], Step [14/149], Loss: 0.6773\n",
      "Epoch [3/10], Step [15/149], Loss: 0.6746\n",
      "Epoch [3/10], Step [16/149], Loss: 0.6886\n",
      "Epoch [3/10], Step [17/149], Loss: 0.6788\n",
      "Epoch [3/10], Step [18/149], Loss: 0.7029\n",
      "Epoch [3/10], Step [19/149], Loss: 0.6777\n",
      "Epoch [3/10], Step [20/149], Loss: 0.6932\n",
      "Epoch [3/10], Step [21/149], Loss: 0.6883\n",
      "Epoch [3/10], Step [22/149], Loss: 0.6835\n",
      "Epoch [3/10], Step [23/149], Loss: 0.6878\n",
      "Epoch [3/10], Step [24/149], Loss: 0.6893\n",
      "Epoch [3/10], Step [25/149], Loss: 0.6813\n",
      "Epoch [3/10], Step [26/149], Loss: 0.6778\n",
      "Epoch [3/10], Step [27/149], Loss: 0.6868\n",
      "Epoch [3/10], Step [28/149], Loss: 0.6741\n",
      "Epoch [3/10], Step [29/149], Loss: 0.6684\n",
      "Epoch [3/10], Step [30/149], Loss: 0.6716\n",
      "Epoch [3/10], Step [31/149], Loss: 0.6765\n",
      "Epoch [3/10], Step [32/149], Loss: 0.6836\n",
      "Epoch [3/10], Step [33/149], Loss: 0.7036\n",
      "Epoch [3/10], Step [34/149], Loss: 0.6930\n",
      "Epoch [3/10], Step [35/149], Loss: 0.6937\n",
      "Epoch [3/10], Step [36/149], Loss: 0.6977\n",
      "Epoch [3/10], Step [37/149], Loss: 0.6700\n",
      "Epoch [3/10], Step [38/149], Loss: 0.6871\n",
      "Epoch [3/10], Step [39/149], Loss: 0.6930\n",
      "Epoch [3/10], Step [40/149], Loss: 0.6929\n",
      "Epoch [3/10], Step [41/149], Loss: 0.6843\n",
      "Epoch [3/10], Step [42/149], Loss: 0.6765\n",
      "Epoch [3/10], Step [43/149], Loss: 0.6913\n",
      "Epoch [3/10], Step [44/149], Loss: 0.6943\n",
      "Epoch [3/10], Step [45/149], Loss: 0.6878\n",
      "Epoch [3/10], Step [46/149], Loss: 0.6861\n",
      "Epoch [3/10], Step [47/149], Loss: 0.6914\n",
      "Epoch [3/10], Step [48/149], Loss: 0.6899\n",
      "Epoch [3/10], Step [49/149], Loss: 0.6847\n",
      "Epoch [3/10], Step [50/149], Loss: 0.6858\n",
      "Epoch [3/10], Step [51/149], Loss: 0.6907\n",
      "Epoch [3/10], Step [52/149], Loss: 0.6802\n",
      "Epoch [3/10], Step [53/149], Loss: 0.6876\n",
      "Epoch [3/10], Step [54/149], Loss: 0.6910\n",
      "Epoch [3/10], Step [55/149], Loss: 0.6977\n",
      "Epoch [3/10], Step [56/149], Loss: 0.6814\n",
      "Epoch [3/10], Step [57/149], Loss: 0.6837\n",
      "Epoch [3/10], Step [58/149], Loss: 0.6905\n",
      "Epoch [3/10], Step [59/149], Loss: 0.6763\n",
      "Epoch [3/10], Step [60/149], Loss: 0.6837\n",
      "Epoch [3/10], Step [61/149], Loss: 0.6921\n",
      "Epoch [3/10], Step [62/149], Loss: 0.6846\n",
      "Epoch [3/10], Step [63/149], Loss: 0.6850\n",
      "Epoch [3/10], Step [64/149], Loss: 0.6917\n",
      "Epoch [3/10], Step [65/149], Loss: 0.6713\n",
      "Epoch [3/10], Step [66/149], Loss: 0.6919\n",
      "Epoch [3/10], Step [67/149], Loss: 0.6958\n",
      "Epoch [3/10], Step [68/149], Loss: 0.6944\n",
      "Epoch [3/10], Step [69/149], Loss: 0.6971\n",
      "Epoch [3/10], Step [70/149], Loss: 0.6720\n",
      "Epoch [3/10], Step [71/149], Loss: 0.6848\n",
      "Epoch [3/10], Step [72/149], Loss: 0.6957\n",
      "Epoch [3/10], Step [73/149], Loss: 0.6808\n",
      "Epoch [3/10], Step [74/149], Loss: 0.6862\n",
      "Epoch [3/10], Step [75/149], Loss: 0.6981\n",
      "Epoch [3/10], Step [76/149], Loss: 0.6951\n",
      "Epoch [3/10], Step [77/149], Loss: 0.6786\n",
      "Epoch [3/10], Step [78/149], Loss: 0.6822\n",
      "Epoch [3/10], Step [79/149], Loss: 0.6896\n",
      "Epoch [3/10], Step [80/149], Loss: 0.6771\n",
      "Epoch [3/10], Step [81/149], Loss: 0.7100\n",
      "Epoch [3/10], Step [82/149], Loss: 0.6947\n",
      "Epoch [3/10], Step [83/149], Loss: 0.6923\n",
      "Epoch [3/10], Step [84/149], Loss: 0.6772\n",
      "Epoch [3/10], Step [85/149], Loss: 0.7022\n",
      "Epoch [3/10], Step [86/149], Loss: 0.6946\n",
      "Epoch [3/10], Step [87/149], Loss: 0.6798\n",
      "Epoch [3/10], Step [88/149], Loss: 0.6925\n",
      "Epoch [3/10], Step [89/149], Loss: 0.6903\n",
      "Epoch [3/10], Step [90/149], Loss: 0.7062\n",
      "Epoch [3/10], Step [91/149], Loss: 0.6917\n",
      "Epoch [3/10], Step [92/149], Loss: 0.6976\n",
      "Epoch [3/10], Step [93/149], Loss: 0.6977\n",
      "Epoch [3/10], Step [94/149], Loss: 0.6882\n",
      "Epoch [3/10], Step [95/149], Loss: 0.6885\n",
      "Epoch [3/10], Step [96/149], Loss: 0.6790\n",
      "Epoch [3/10], Step [97/149], Loss: 0.6682\n",
      "Epoch [3/10], Step [98/149], Loss: 0.6835\n",
      "Epoch [3/10], Step [99/149], Loss: 0.6904\n",
      "Epoch [3/10], Step [100/149], Loss: 0.6917\n",
      "Epoch [3/10], Step [101/149], Loss: 0.6990\n",
      "Epoch [3/10], Step [102/149], Loss: 0.6957\n",
      "Epoch [3/10], Step [103/149], Loss: 0.6825\n",
      "Epoch [3/10], Step [104/149], Loss: 0.7027\n",
      "Epoch [3/10], Step [105/149], Loss: 0.6849\n",
      "Epoch [3/10], Step [106/149], Loss: 0.7081\n",
      "Epoch [3/10], Step [107/149], Loss: 0.6860\n",
      "Epoch [3/10], Step [108/149], Loss: 0.6742\n",
      "Epoch [3/10], Step [109/149], Loss: 0.6767\n",
      "Epoch [3/10], Step [110/149], Loss: 0.6910\n",
      "Epoch [3/10], Step [111/149], Loss: 0.6878\n",
      "Epoch [3/10], Step [112/149], Loss: 0.6815\n",
      "Epoch [3/10], Step [113/149], Loss: 0.6860\n",
      "Epoch [3/10], Step [114/149], Loss: 0.6925\n",
      "Epoch [3/10], Step [115/149], Loss: 0.6982\n",
      "Epoch [3/10], Step [116/149], Loss: 0.6854\n",
      "Epoch [3/10], Step [117/149], Loss: 0.6852\n",
      "Epoch [3/10], Step [118/149], Loss: 0.6973\n",
      "Epoch [3/10], Step [119/149], Loss: 0.6845\n",
      "Epoch [3/10], Step [120/149], Loss: 0.6856\n",
      "Epoch [3/10], Step [121/149], Loss: 0.6802\n",
      "Epoch [3/10], Step [122/149], Loss: 0.6871\n",
      "Epoch [3/10], Step [123/149], Loss: 0.6830\n",
      "Epoch [3/10], Step [124/149], Loss: 0.6847\n",
      "Epoch [3/10], Step [125/149], Loss: 0.6889\n",
      "Epoch [3/10], Step [126/149], Loss: 0.6866\n",
      "Epoch [3/10], Step [127/149], Loss: 0.6777\n",
      "Epoch [3/10], Step [128/149], Loss: 0.7021\n",
      "Epoch [3/10], Step [129/149], Loss: 0.6941\n",
      "Epoch [3/10], Step [130/149], Loss: 0.6967\n",
      "Epoch [3/10], Step [131/149], Loss: 0.6766\n",
      "Epoch [3/10], Step [132/149], Loss: 0.6925\n",
      "Epoch [3/10], Step [133/149], Loss: 0.6898\n",
      "Epoch [3/10], Step [134/149], Loss: 0.6756\n",
      "Epoch [3/10], Step [135/149], Loss: 0.6868\n",
      "Epoch [3/10], Step [136/149], Loss: 0.6998\n",
      "Epoch [3/10], Step [137/149], Loss: 0.7040\n",
      "Epoch [3/10], Step [138/149], Loss: 0.6795\n",
      "Epoch [3/10], Step [139/149], Loss: 0.6964\n",
      "Epoch [3/10], Step [140/149], Loss: 0.6936\n",
      "Epoch [3/10], Step [141/149], Loss: 0.6782\n",
      "Epoch [3/10], Step [142/149], Loss: 0.6889\n",
      "Epoch [3/10], Step [143/149], Loss: 0.7046\n",
      "Epoch [3/10], Step [144/149], Loss: 0.6868\n",
      "Epoch [3/10], Step [145/149], Loss: 0.6862\n",
      "Epoch [3/10], Step [146/149], Loss: 0.6825\n",
      "Epoch [3/10], Step [147/149], Loss: 0.7002\n",
      "Epoch [3/10], Step [148/149], Loss: 0.6951\n",
      "Epoch [3/10], Step [149/149], Loss: 0.7076\n",
      "Epoch [3/10] and the best match:\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1])\n",
      "tensor([[ 1.1817e-02,  5.4034e-02],\n",
      "        [-2.2369e-04,  6.6114e-02],\n",
      "        [ 6.4219e-02, -9.6773e-03],\n",
      "        [ 2.1324e-01, -2.8972e-01],\n",
      "        [ 8.6928e-02,  4.0331e-02],\n",
      "        [-1.4466e-02,  5.9606e-02],\n",
      "        [ 1.4246e-01, -1.9611e-02],\n",
      "        [ 1.0406e-01,  3.5527e-03],\n",
      "        [ 2.2466e-01, -1.3414e-01],\n",
      "        [ 5.2055e-02,  1.9629e-02],\n",
      "        [ 1.5369e-01, -6.3242e-02],\n",
      "        [ 3.4921e-02,  9.2282e-03],\n",
      "        [ 1.3578e-02,  5.2257e-03],\n",
      "        [-4.0246e-02,  7.5354e-02],\n",
      "        [ 1.1862e-01, -1.9531e-02],\n",
      "        [ 2.2510e-01, -5.4923e-02],\n",
      "        [ 3.0871e-02,  4.3737e-02],\n",
      "        [ 4.6631e-02,  4.8125e-02],\n",
      "        [ 2.5066e-01, -8.4056e-02],\n",
      "        [ 9.4953e-02, -3.3486e-02],\n",
      "        [ 3.5455e-02,  2.3192e-02],\n",
      "        [-3.2828e-02,  9.7917e-02],\n",
      "        [ 8.6910e-02,  2.6653e-02],\n",
      "        [ 3.2565e-02,  3.4998e-02],\n",
      "        [ 2.1719e-01, -1.0186e-01],\n",
      "        [ 3.9569e-02, -1.6216e-02],\n",
      "        [ 2.3281e-01, -8.5912e-02],\n",
      "        [ 1.4496e-01, -3.4314e-02],\n",
      "        [ 1.7765e-02,  4.4397e-02],\n",
      "        [ 1.1569e-03,  2.8543e-02],\n",
      "        [ 8.7543e-02,  2.5407e-02],\n",
      "        [ 2.1784e-01, -9.6962e-02],\n",
      "        [ 2.0059e-01, -1.3961e-01],\n",
      "        [ 1.4044e-01, -7.2673e-02],\n",
      "        [ 1.6601e-01, -2.2572e-02],\n",
      "        [ 1.1348e-01, -2.6193e-02],\n",
      "        [ 9.3081e-02, -7.7179e-02],\n",
      "        [ 1.4692e-01,  1.2814e-02],\n",
      "        [ 3.9011e-02,  6.4619e-02],\n",
      "        [ 1.3439e-01,  5.5901e-02],\n",
      "        [ 1.2586e-01, -5.0323e-02],\n",
      "        [ 1.1530e-01,  2.0445e-02],\n",
      "        [-2.3913e-02,  3.7550e-02],\n",
      "        [ 7.1088e-02, -4.5484e-02],\n",
      "        [ 1.3594e-01, -9.6249e-03],\n",
      "        [ 7.5305e-02, -7.6505e-04],\n",
      "        [ 9.4975e-02, -3.2773e-02],\n",
      "        [-2.7525e-03,  3.4200e-02],\n",
      "        [ 7.7360e-03,  2.1169e-02],\n",
      "        [ 8.0761e-02, -1.1392e-02],\n",
      "        [-5.0267e-03,  9.1366e-02],\n",
      "        [ 6.5808e-02,  9.3232e-02],\n",
      "        [ 1.3009e-01, -2.6508e-02],\n",
      "        [ 8.3572e-02,  1.2673e-03],\n",
      "        [ 1.3272e-01, -6.6814e-02],\n",
      "        [ 1.5263e-01, -4.6057e-03],\n",
      "        [ 9.4517e-02, -9.4520e-04],\n",
      "        [ 2.9162e-02,  6.1164e-02],\n",
      "        [ 2.1926e-01, -1.0912e-01],\n",
      "        [ 9.2502e-02,  5.4224e-02],\n",
      "        [ 1.6673e-01, -5.4152e-02],\n",
      "        [ 7.7293e-02,  4.0842e-02],\n",
      "        [ 8.1901e-02, -1.6960e-02],\n",
      "        [ 1.0151e-01, -1.7749e-02],\n",
      "        [ 1.2402e-01, -3.7452e-02],\n",
      "        [ 1.6308e-01, -7.1088e-02],\n",
      "        [ 9.2274e-02, -5.4146e-02],\n",
      "        [ 4.4511e-02,  1.1818e-02],\n",
      "        [ 7.6994e-02, -3.5834e-02],\n",
      "        [ 7.1752e-02,  2.2374e-03],\n",
      "        [ 2.0284e-02,  3.5064e-02],\n",
      "        [ 5.3015e-02,  3.0201e-02],\n",
      "        [ 1.2255e-01, -1.7075e-02],\n",
      "        [ 7.6012e-02,  3.0134e-02],\n",
      "        [ 8.9839e-02, -2.8815e-02],\n",
      "        [ 1.4594e-01, -3.3729e-02],\n",
      "        [ 8.3273e-02,  3.1135e-02],\n",
      "        [ 1.3306e-01, -6.1510e-02],\n",
      "        [ 5.4631e-02,  6.5108e-02],\n",
      "        [ 1.3730e-02,  9.3901e-02],\n",
      "        [-2.1065e-02,  3.5893e-02],\n",
      "        [ 9.4591e-02, -2.2983e-02],\n",
      "        [ 9.6085e-02,  3.9674e-02],\n",
      "        [ 5.9224e-02,  6.5786e-03],\n",
      "        [ 1.6550e-02,  4.0319e-02],\n",
      "        [ 1.6584e-01, -7.1590e-02],\n",
      "        [ 1.6031e-01, -3.4625e-02],\n",
      "        [ 5.0523e-02, -4.2141e-03],\n",
      "        [-2.0195e-02,  8.6237e-02],\n",
      "        [ 6.5036e-02,  4.4601e-03],\n",
      "        [ 1.1144e-01, -2.1621e-02],\n",
      "        [ 7.5292e-02,  4.0273e-02],\n",
      "        [ 9.3313e-02, -5.1485e-02],\n",
      "        [ 7.6587e-02,  2.5538e-02],\n",
      "        [ 1.4124e-01, -1.0240e-01],\n",
      "        [ 1.0113e-01,  1.0627e-02],\n",
      "        [ 9.9417e-02, -1.6913e-02],\n",
      "        [ 1.9405e-01, -1.8241e-02],\n",
      "        [ 1.4777e-01, -1.3345e-02],\n",
      "        [ 6.4211e-02, -2.3709e-02],\n",
      "        [ 2.2704e-01, -1.3656e-01],\n",
      "        [ 5.9102e-02,  2.3276e-02],\n",
      "        [-1.0390e-02,  6.8570e-02],\n",
      "        [ 1.1418e-01,  2.2833e-02],\n",
      "        [-1.5763e-02,  4.8573e-02],\n",
      "        [ 5.2950e-02,  2.6779e-02],\n",
      "        [ 1.1483e-01,  2.2349e-02],\n",
      "        [ 6.5823e-02,  2.3517e-03],\n",
      "        [ 2.8163e-02,  2.0724e-03],\n",
      "        [ 2.0346e-01, -8.7415e-02],\n",
      "        [ 1.5431e-01, -4.8798e-02],\n",
      "        [-7.6639e-03,  4.8176e-02],\n",
      "        [ 7.5166e-02, -3.9035e-02],\n",
      "        [-5.9453e-02,  1.0917e-01],\n",
      "        [ 1.0431e-01, -5.2991e-02],\n",
      "        [ 4.8878e-02,  2.7189e-02],\n",
      "        [ 1.1554e-01, -3.2493e-03],\n",
      "        [ 1.2735e-01,  3.4808e-03],\n",
      "        [ 8.4809e-02,  7.4684e-03],\n",
      "        [ 2.1986e-01, -7.3527e-02],\n",
      "        [-3.1293e-02,  8.8288e-02],\n",
      "        [ 2.3040e-01, -1.7664e-01],\n",
      "        [ 7.1795e-02, -1.1157e-03],\n",
      "        [ 3.7312e-02,  3.7076e-03],\n",
      "        [-2.5520e-03,  2.4835e-02],\n",
      "        [ 1.1127e-01, -2.6770e-02],\n",
      "        [ 1.9684e-02,  1.9696e-02],\n",
      "        [-3.7885e-02,  9.1575e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [1/149], Loss: 0.6859\n",
      "Epoch [4/10], Step [2/149], Loss: 0.6894\n",
      "Epoch [4/10], Step [3/149], Loss: 0.6674\n",
      "Epoch [4/10], Step [4/149], Loss: 0.6837\n",
      "Epoch [4/10], Step [5/149], Loss: 0.6776\n",
      "Epoch [4/10], Step [6/149], Loss: 0.6794\n",
      "Epoch [4/10], Step [7/149], Loss: 0.6755\n",
      "Epoch [4/10], Step [8/149], Loss: 0.6809\n",
      "Epoch [4/10], Step [9/149], Loss: 0.6844\n",
      "Epoch [4/10], Step [10/149], Loss: 0.6812\n",
      "Epoch [4/10], Step [11/149], Loss: 0.6736\n",
      "Epoch [4/10], Step [12/149], Loss: 0.6817\n",
      "Epoch [4/10], Step [13/149], Loss: 0.6647\n",
      "Epoch [4/10], Step [14/149], Loss: 0.6669\n",
      "Epoch [4/10], Step [15/149], Loss: 0.6763\n",
      "Epoch [4/10], Step [16/149], Loss: 0.6802\n",
      "Epoch [4/10], Step [17/149], Loss: 0.6796\n",
      "Epoch [4/10], Step [18/149], Loss: 0.6850\n",
      "Epoch [4/10], Step [19/149], Loss: 0.7043\n",
      "Epoch [4/10], Step [20/149], Loss: 0.6752\n",
      "Epoch [4/10], Step [21/149], Loss: 0.6772\n",
      "Epoch [4/10], Step [22/149], Loss: 0.6635\n",
      "Epoch [4/10], Step [23/149], Loss: 0.6660\n",
      "Epoch [4/10], Step [24/149], Loss: 0.6733\n",
      "Epoch [4/10], Step [25/149], Loss: 0.6729\n",
      "Epoch [4/10], Step [26/149], Loss: 0.6634\n",
      "Epoch [4/10], Step [27/149], Loss: 0.6824\n",
      "Epoch [4/10], Step [28/149], Loss: 0.6692\n",
      "Epoch [4/10], Step [29/149], Loss: 0.6673\n",
      "Epoch [4/10], Step [30/149], Loss: 0.6561\n",
      "Epoch [4/10], Step [31/149], Loss: 0.6777\n",
      "Epoch [4/10], Step [32/149], Loss: 0.6949\n",
      "Epoch [4/10], Step [33/149], Loss: 0.6934\n",
      "Epoch [4/10], Step [34/149], Loss: 0.6593\n",
      "Epoch [4/10], Step [35/149], Loss: 0.6654\n",
      "Epoch [4/10], Step [36/149], Loss: 0.6776\n",
      "Epoch [4/10], Step [37/149], Loss: 0.6480\n",
      "Epoch [4/10], Step [38/149], Loss: 0.6515\n",
      "Epoch [4/10], Step [39/149], Loss: 0.6576\n",
      "Epoch [4/10], Step [40/149], Loss: 0.6671\n",
      "Epoch [4/10], Step [41/149], Loss: 0.6949\n",
      "Epoch [4/10], Step [42/149], Loss: 0.6763\n",
      "Epoch [4/10], Step [43/149], Loss: 0.6965\n",
      "Epoch [4/10], Step [44/149], Loss: 0.6559\n",
      "Epoch [4/10], Step [45/149], Loss: 0.6699\n",
      "Epoch [4/10], Step [46/149], Loss: 0.6858\n",
      "Epoch [4/10], Step [47/149], Loss: 0.6908\n",
      "Epoch [4/10], Step [48/149], Loss: 0.6837\n",
      "Epoch [4/10], Step [49/149], Loss: 0.6962\n",
      "Epoch [4/10], Step [50/149], Loss: 0.6460\n",
      "Epoch [4/10], Step [51/149], Loss: 0.6699\n",
      "Epoch [4/10], Step [52/149], Loss: 0.6897\n",
      "Epoch [4/10], Step [53/149], Loss: 0.6901\n",
      "Epoch [4/10], Step [54/149], Loss: 0.6447\n",
      "Epoch [4/10], Step [55/149], Loss: 0.6811\n",
      "Epoch [4/10], Step [56/149], Loss: 0.6654\n",
      "Epoch [4/10], Step [57/149], Loss: 0.6972\n",
      "Epoch [4/10], Step [58/149], Loss: 0.7072\n",
      "Epoch [4/10], Step [59/149], Loss: 0.6752\n",
      "Epoch [4/10], Step [60/149], Loss: 0.6674\n",
      "Epoch [4/10], Step [61/149], Loss: 0.6638\n",
      "Epoch [4/10], Step [62/149], Loss: 0.6678\n",
      "Epoch [4/10], Step [63/149], Loss: 0.6888\n",
      "Epoch [4/10], Step [64/149], Loss: 0.6655\n",
      "Epoch [4/10], Step [65/149], Loss: 0.6790\n",
      "Epoch [4/10], Step [66/149], Loss: 0.6843\n",
      "Epoch [4/10], Step [67/149], Loss: 0.6704\n",
      "Epoch [4/10], Step [68/149], Loss: 0.6811\n",
      "Epoch [4/10], Step [69/149], Loss: 0.6803\n",
      "Epoch [4/10], Step [70/149], Loss: 0.6919\n",
      "Epoch [4/10], Step [71/149], Loss: 0.6667\n",
      "Epoch [4/10], Step [72/149], Loss: 0.6629\n",
      "Epoch [4/10], Step [73/149], Loss: 0.6594\n",
      "Epoch [4/10], Step [74/149], Loss: 0.6867\n",
      "Epoch [4/10], Step [75/149], Loss: 0.6740\n",
      "Epoch [4/10], Step [76/149], Loss: 0.6764\n",
      "Epoch [4/10], Step [77/149], Loss: 0.6855\n",
      "Epoch [4/10], Step [78/149], Loss: 0.6396\n",
      "Epoch [4/10], Step [79/149], Loss: 0.6771\n",
      "Epoch [4/10], Step [80/149], Loss: 0.6475\n",
      "Epoch [4/10], Step [81/149], Loss: 0.6683\n",
      "Epoch [4/10], Step [82/149], Loss: 0.6706\n",
      "Epoch [4/10], Step [83/149], Loss: 0.6815\n",
      "Epoch [4/10], Step [84/149], Loss: 0.6496\n",
      "Epoch [4/10], Step [85/149], Loss: 0.6913\n",
      "Epoch [4/10], Step [86/149], Loss: 0.6521\n",
      "Epoch [4/10], Step [87/149], Loss: 0.6721\n",
      "Epoch [4/10], Step [88/149], Loss: 0.6425\n",
      "Epoch [4/10], Step [89/149], Loss: 0.6773\n",
      "Epoch [4/10], Step [90/149], Loss: 0.6684\n",
      "Epoch [4/10], Step [91/149], Loss: 0.7079\n",
      "Epoch [4/10], Step [92/149], Loss: 0.6544\n",
      "Epoch [4/10], Step [93/149], Loss: 0.6613\n",
      "Epoch [4/10], Step [94/149], Loss: 0.7014\n",
      "Epoch [4/10], Step [95/149], Loss: 0.6586\n",
      "Epoch [4/10], Step [96/149], Loss: 0.6843\n",
      "Epoch [4/10], Step [97/149], Loss: 0.6704\n",
      "Epoch [4/10], Step [98/149], Loss: 0.6508\n",
      "Epoch [4/10], Step [99/149], Loss: 0.6715\n",
      "Epoch [4/10], Step [100/149], Loss: 0.6960\n",
      "Epoch [4/10], Step [101/149], Loss: 0.6922\n",
      "Epoch [4/10], Step [102/149], Loss: 0.6792\n",
      "Epoch [4/10], Step [103/149], Loss: 0.6531\n",
      "Epoch [4/10], Step [104/149], Loss: 0.6521\n",
      "Epoch [4/10], Step [105/149], Loss: 0.6703\n",
      "Epoch [4/10], Step [106/149], Loss: 0.6767\n",
      "Epoch [4/10], Step [107/149], Loss: 0.6283\n",
      "Epoch [4/10], Step [108/149], Loss: 0.6842\n",
      "Epoch [4/10], Step [109/149], Loss: 0.6951\n",
      "Epoch [4/10], Step [110/149], Loss: 0.6791\n",
      "Epoch [4/10], Step [111/149], Loss: 0.6820\n",
      "Epoch [4/10], Step [112/149], Loss: 0.6817\n",
      "Epoch [4/10], Step [113/149], Loss: 0.6770\n",
      "Epoch [4/10], Step [114/149], Loss: 0.6754\n",
      "Epoch [4/10], Step [115/149], Loss: 0.6591\n",
      "Epoch [4/10], Step [116/149], Loss: 0.6769\n",
      "Epoch [4/10], Step [117/149], Loss: 0.6638\n",
      "Epoch [4/10], Step [118/149], Loss: 0.6507\n",
      "Epoch [4/10], Step [119/149], Loss: 0.6941\n",
      "Epoch [4/10], Step [120/149], Loss: 0.6738\n",
      "Epoch [4/10], Step [121/149], Loss: 0.6788\n",
      "Epoch [4/10], Step [122/149], Loss: 0.6750\n",
      "Epoch [4/10], Step [123/149], Loss: 0.6698\n",
      "Epoch [4/10], Step [124/149], Loss: 0.6847\n",
      "Epoch [4/10], Step [125/149], Loss: 0.7138\n",
      "Epoch [4/10], Step [126/149], Loss: 0.7118\n",
      "Epoch [4/10], Step [127/149], Loss: 0.6737\n",
      "Epoch [4/10], Step [128/149], Loss: 0.6851\n",
      "Epoch [4/10], Step [129/149], Loss: 0.6774\n",
      "Epoch [4/10], Step [130/149], Loss: 0.6855\n",
      "Epoch [4/10], Step [131/149], Loss: 0.6693\n",
      "Epoch [4/10], Step [132/149], Loss: 0.6730\n",
      "Epoch [4/10], Step [133/149], Loss: 0.6938\n",
      "Epoch [4/10], Step [134/149], Loss: 0.6773\n",
      "Epoch [4/10], Step [135/149], Loss: 0.6827\n",
      "Epoch [4/10], Step [136/149], Loss: 0.6803\n",
      "Epoch [4/10], Step [137/149], Loss: 0.6716\n",
      "Epoch [4/10], Step [138/149], Loss: 0.6680\n",
      "Epoch [4/10], Step [139/149], Loss: 0.6854\n",
      "Epoch [4/10], Step [140/149], Loss: 0.6702\n",
      "Epoch [4/10], Step [141/149], Loss: 0.6297\n",
      "Epoch [4/10], Step [142/149], Loss: 0.6508\n",
      "Epoch [4/10], Step [143/149], Loss: 0.6384\n",
      "Epoch [4/10], Step [144/149], Loss: 0.6864\n",
      "Epoch [4/10], Step [145/149], Loss: 0.6760\n",
      "Epoch [4/10], Step [146/149], Loss: 0.6864\n",
      "Epoch [4/10], Step [147/149], Loss: 0.6671\n",
      "Epoch [4/10], Step [148/149], Loss: 0.6742\n",
      "Epoch [4/10], Step [149/149], Loss: 0.6139\n",
      "Epoch [4/10] and the best match:\n",
      "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1])\n",
      "tensor([[-0.0424,  0.0316],\n",
      "        [-0.1265,  0.1893],\n",
      "        [ 0.0050,  0.0604],\n",
      "        [ 0.1905, -0.0751],\n",
      "        [ 0.1094, -0.0915],\n",
      "        [ 0.4705, -0.3358],\n",
      "        [ 0.1151, -0.1269],\n",
      "        [ 0.2293, -0.2448],\n",
      "        [ 0.4713, -0.6176],\n",
      "        [ 0.2487, -0.0953],\n",
      "        [ 0.1981, -0.2244],\n",
      "        [ 0.0791, -0.0904],\n",
      "        [ 0.2746, -0.1087],\n",
      "        [ 0.2154, -0.2194],\n",
      "        [ 0.4251, -0.2826],\n",
      "        [ 0.2651, -0.2167],\n",
      "        [-0.0123, -0.0966],\n",
      "        [-0.1669,  0.1253],\n",
      "        [ 0.1201, -0.2445],\n",
      "        [ 0.0214,  0.0777]], grad_fn=<AddmmBackward0>)\n",
      "Epoch [5/10], Step [1/149], Loss: 0.6296\n",
      "Epoch [5/10], Step [2/149], Loss: 0.6164\n",
      "Epoch [5/10], Step [3/149], Loss: 0.6219\n",
      "Epoch [5/10], Step [4/149], Loss: 0.6256\n",
      "Epoch [5/10], Step [5/149], Loss: 0.6462\n",
      "Epoch [5/10], Step [6/149], Loss: 0.6656\n",
      "Epoch [5/10], Step [7/149], Loss: 0.6335\n",
      "Epoch [5/10], Step [8/149], Loss: 0.6526\n",
      "Epoch [5/10], Step [9/149], Loss: 0.6147\n",
      "Epoch [5/10], Step [10/149], Loss: 0.6284\n",
      "Epoch [5/10], Step [11/149], Loss: 0.6563\n",
      "Epoch [5/10], Step [12/149], Loss: 0.6430\n",
      "Epoch [5/10], Step [13/149], Loss: 0.6553\n",
      "Epoch [5/10], Step [14/149], Loss: 0.6199\n",
      "Epoch [5/10], Step [15/149], Loss: 0.6694\n",
      "Epoch [5/10], Step [16/149], Loss: 0.6526\n",
      "Epoch [5/10], Step [17/149], Loss: 0.6348\n",
      "Epoch [5/10], Step [18/149], Loss: 0.6826\n",
      "Epoch [5/10], Step [19/149], Loss: 0.6375\n",
      "Epoch [5/10], Step [20/149], Loss: 0.6325\n",
      "Epoch [5/10], Step [21/149], Loss: 0.6320\n",
      "Epoch [5/10], Step [22/149], Loss: 0.7191\n",
      "Epoch [5/10], Step [23/149], Loss: 0.6435\n",
      "Epoch [5/10], Step [24/149], Loss: 0.6380\n",
      "Epoch [5/10], Step [25/149], Loss: 0.6728\n",
      "Epoch [5/10], Step [26/149], Loss: 0.6061\n",
      "Epoch [5/10], Step [27/149], Loss: 0.6532\n",
      "Epoch [5/10], Step [28/149], Loss: 0.6321\n",
      "Epoch [5/10], Step [29/149], Loss: 0.6317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [30/149], Loss: 0.6396\n",
      "Epoch [5/10], Step [31/149], Loss: 0.6521\n",
      "Epoch [5/10], Step [32/149], Loss: 0.6422\n",
      "Epoch [5/10], Step [33/149], Loss: 0.6730\n",
      "Epoch [5/10], Step [34/149], Loss: 0.6617\n",
      "Epoch [5/10], Step [35/149], Loss: 0.6707\n",
      "Epoch [5/10], Step [36/149], Loss: 0.6596\n",
      "Epoch [5/10], Step [37/149], Loss: 0.6701\n",
      "Epoch [5/10], Step [38/149], Loss: 0.6535\n",
      "Epoch [5/10], Step [39/149], Loss: 0.6100\n",
      "Epoch [5/10], Step [40/149], Loss: 0.6363\n",
      "Epoch [5/10], Step [41/149], Loss: 0.6555\n",
      "Epoch [5/10], Step [42/149], Loss: 0.6731\n",
      "Epoch [5/10], Step [43/149], Loss: 0.6028\n",
      "Epoch [5/10], Step [44/149], Loss: 0.6303\n",
      "Epoch [5/10], Step [45/149], Loss: 0.6457\n",
      "Epoch [5/10], Step [46/149], Loss: 0.6127\n",
      "Epoch [5/10], Step [47/149], Loss: 0.6603\n",
      "Epoch [5/10], Step [48/149], Loss: 0.6369\n",
      "Epoch [5/10], Step [49/149], Loss: 0.6320\n",
      "Epoch [5/10], Step [50/149], Loss: 0.6334\n",
      "Epoch [5/10], Step [51/149], Loss: 0.6404\n",
      "Epoch [5/10], Step [52/149], Loss: 0.6026\n",
      "Epoch [5/10], Step [53/149], Loss: 0.6360\n",
      "Epoch [5/10], Step [54/149], Loss: 0.6419\n",
      "Epoch [5/10], Step [55/149], Loss: 0.6624\n",
      "Epoch [5/10], Step [56/149], Loss: 0.6534\n",
      "Epoch [5/10], Step [57/149], Loss: 0.6066\n",
      "Epoch [5/10], Step [58/149], Loss: 0.6343\n",
      "Epoch [5/10], Step [59/149], Loss: 0.6002\n",
      "Epoch [5/10], Step [60/149], Loss: 0.6306\n",
      "Epoch [5/10], Step [61/149], Loss: 0.5771\n",
      "Epoch [5/10], Step [62/149], Loss: 0.6592\n",
      "Epoch [5/10], Step [63/149], Loss: 0.6389\n",
      "Epoch [5/10], Step [64/149], Loss: 0.6269\n",
      "Epoch [5/10], Step [65/149], Loss: 0.5980\n",
      "Epoch [5/10], Step [66/149], Loss: 0.6393\n",
      "Epoch [5/10], Step [67/149], Loss: 0.6262\n",
      "Epoch [5/10], Step [68/149], Loss: 0.6677\n",
      "Epoch [5/10], Step [69/149], Loss: 0.5965\n",
      "Epoch [5/10], Step [70/149], Loss: 0.6429\n",
      "Epoch [5/10], Step [71/149], Loss: 0.6222\n",
      "Epoch [5/10], Step [72/149], Loss: 0.6453\n",
      "Epoch [5/10], Step [73/149], Loss: 0.6695\n",
      "Epoch [5/10], Step [74/149], Loss: 0.6246\n",
      "Epoch [5/10], Step [75/149], Loss: 0.6302\n",
      "Epoch [5/10], Step [76/149], Loss: 0.6650\n",
      "Epoch [5/10], Step [77/149], Loss: 0.6114\n",
      "Epoch [5/10], Step [78/149], Loss: 0.6188\n",
      "Epoch [5/10], Step [79/149], Loss: 0.6248\n",
      "Epoch [5/10], Step [80/149], Loss: 0.6266\n",
      "Epoch [5/10], Step [81/149], Loss: 0.6474\n",
      "Epoch [5/10], Step [82/149], Loss: 0.6412\n",
      "Epoch [5/10], Step [83/149], Loss: 0.6318\n",
      "Epoch [5/10], Step [84/149], Loss: 0.6530\n",
      "Epoch [5/10], Step [85/149], Loss: 0.5852\n",
      "Epoch [5/10], Step [86/149], Loss: 0.6133\n",
      "Epoch [5/10], Step [87/149], Loss: 0.6373\n",
      "Epoch [5/10], Step [88/149], Loss: 0.6634\n",
      "Epoch [5/10], Step [89/149], Loss: 0.6129\n",
      "Epoch [5/10], Step [90/149], Loss: 0.6135\n",
      "Epoch [5/10], Step [91/149], Loss: 0.6389\n",
      "Epoch [5/10], Step [92/149], Loss: 0.6725\n",
      "Epoch [5/10], Step [93/149], Loss: 0.6215\n",
      "Epoch [5/10], Step [94/149], Loss: 0.6562\n",
      "Epoch [5/10], Step [95/149], Loss: 0.6475\n",
      "Epoch [5/10], Step [96/149], Loss: 0.6213\n",
      "Epoch [5/10], Step [97/149], Loss: 0.6462\n",
      "Epoch [5/10], Step [98/149], Loss: 0.5879\n",
      "Epoch [5/10], Step [99/149], Loss: 0.6120\n",
      "Epoch [5/10], Step [100/149], Loss: 0.6243\n",
      "Epoch [5/10], Step [101/149], Loss: 0.6607\n",
      "Epoch [5/10], Step [102/149], Loss: 0.6332\n",
      "Epoch [5/10], Step [103/149], Loss: 0.6414\n",
      "Epoch [5/10], Step [104/149], Loss: 0.6399\n",
      "Epoch [5/10], Step [105/149], Loss: 0.6816\n",
      "Epoch [5/10], Step [106/149], Loss: 0.6084\n",
      "Epoch [5/10], Step [107/149], Loss: 0.6573\n",
      "Epoch [5/10], Step [108/149], Loss: 0.6534\n",
      "Epoch [5/10], Step [109/149], Loss: 0.6120\n",
      "Epoch [5/10], Step [110/149], Loss: 0.6451\n",
      "Epoch [5/10], Step [111/149], Loss: 0.6277\n",
      "Epoch [5/10], Step [112/149], Loss: 0.6396\n",
      "Epoch [5/10], Step [113/149], Loss: 0.6240\n",
      "Epoch [5/10], Step [114/149], Loss: 0.6168\n",
      "Epoch [5/10], Step [115/149], Loss: 0.6384\n",
      "Epoch [5/10], Step [116/149], Loss: 0.6295\n",
      "Epoch [5/10], Step [117/149], Loss: 0.6375\n",
      "Epoch [5/10], Step [118/149], Loss: 0.6631\n",
      "Epoch [5/10], Step [119/149], Loss: 0.6433\n",
      "Epoch [5/10], Step [120/149], Loss: 0.6224\n",
      "Epoch [5/10], Step [121/149], Loss: 0.6359\n",
      "Epoch [5/10], Step [122/149], Loss: 0.6226\n",
      "Epoch [5/10], Step [123/149], Loss: 0.6608\n",
      "Epoch [5/10], Step [124/149], Loss: 0.6206\n",
      "Epoch [5/10], Step [125/149], Loss: 0.6167\n",
      "Epoch [5/10], Step [126/149], Loss: 0.6269\n",
      "Epoch [5/10], Step [127/149], Loss: 0.6579\n",
      "Epoch [5/10], Step [128/149], Loss: 0.5998\n",
      "Epoch [5/10], Step [129/149], Loss: 0.6065\n",
      "Epoch [5/10], Step [130/149], Loss: 0.6191\n",
      "Epoch [5/10], Step [131/149], Loss: 0.6400\n",
      "Epoch [5/10], Step [132/149], Loss: 0.6459\n",
      "Epoch [5/10], Step [133/149], Loss: 0.6186\n",
      "Epoch [5/10], Step [134/149], Loss: 0.6483\n",
      "Epoch [5/10], Step [135/149], Loss: 0.6127\n",
      "Epoch [5/10], Step [136/149], Loss: 0.5984\n",
      "Epoch [5/10], Step [137/149], Loss: 0.6060\n",
      "Epoch [5/10], Step [138/149], Loss: 0.5960\n",
      "Epoch [5/10], Step [139/149], Loss: 0.5972\n",
      "Epoch [5/10], Step [140/149], Loss: 0.7122\n",
      "Epoch [5/10], Step [141/149], Loss: 0.6131\n",
      "Epoch [5/10], Step [142/149], Loss: 0.6151\n",
      "Epoch [5/10], Step [143/149], Loss: 0.6558\n",
      "Epoch [5/10], Step [144/149], Loss: 0.6055\n",
      "Epoch [5/10], Step [145/149], Loss: 0.6482\n",
      "Epoch [5/10], Step [146/149], Loss: 0.6330\n",
      "Epoch [5/10], Step [147/149], Loss: 0.6432\n",
      "Epoch [5/10], Step [148/149], Loss: 0.6142\n",
      "Epoch [5/10], Step [149/149], Loss: 0.5910\n",
      "Epoch [5/10] and the best match:\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([[-1.3609e-01, -9.1131e-03],\n",
      "        [ 7.0240e-01, -8.0788e-01],\n",
      "        [ 3.8179e-01, -3.9668e-01],\n",
      "        [ 9.1928e-01, -9.9414e-01],\n",
      "        [ 4.5075e-01, -4.5998e-01],\n",
      "        [ 6.1070e-01, -6.8138e-01],\n",
      "        [ 3.2303e-01, -2.6382e-01],\n",
      "        [ 2.6201e-01, -3.4712e-01],\n",
      "        [ 1.4048e-02, -2.4817e-01],\n",
      "        [ 3.7587e-01, -3.2240e-01],\n",
      "        [ 2.8602e-01, -4.1929e-01],\n",
      "        [ 1.3984e-01, -1.4872e-01],\n",
      "        [ 2.8711e-01, -3.2890e-02],\n",
      "        [-9.1984e-02,  1.1474e-01],\n",
      "        [-5.3560e-01,  5.0857e-01],\n",
      "        [-5.4521e-02,  1.3978e-01],\n",
      "        [ 4.9749e-01, -3.3156e-01],\n",
      "        [ 5.0209e-02, -7.7014e-02],\n",
      "        [ 9.2271e-01, -5.6064e-01],\n",
      "        [ 3.3128e-02, -8.1786e-02],\n",
      "        [-1.7894e-01,  1.1821e-01],\n",
      "        [-8.1108e-02,  1.1457e-01],\n",
      "        [ 8.8188e-02, -3.3873e-02],\n",
      "        [ 1.3519e-01, -2.0712e-01],\n",
      "        [ 3.1598e-01, -2.3442e-01],\n",
      "        [-1.0968e-01,  6.5762e-02],\n",
      "        [-1.2193e-01,  1.5560e-01],\n",
      "        [-1.1916e-01, -1.6130e-01],\n",
      "        [ 3.3256e-01, -4.3335e-01],\n",
      "        [-5.4389e-02,  9.9020e-02],\n",
      "        [ 6.2963e-02, -1.6857e-01],\n",
      "        [ 6.9532e-01, -6.9015e-01],\n",
      "        [-2.6090e-01, -4.0113e-02],\n",
      "        [ 6.1084e-01, -5.7265e-01],\n",
      "        [ 3.6682e-01, -4.8526e-01],\n",
      "        [ 2.9333e-01, -2.6392e-01],\n",
      "        [ 4.5961e-01, -4.3610e-01],\n",
      "        [ 1.0042e-01, -7.8050e-02],\n",
      "        [ 5.0928e-02,  1.3836e-02],\n",
      "        [ 6.5378e-01, -7.0587e-01],\n",
      "        [ 1.5768e-01, -1.3631e-01],\n",
      "        [ 1.7900e+00, -1.3772e+00],\n",
      "        [ 7.2171e-01, -5.0051e-01],\n",
      "        [ 3.8187e-01, -3.7020e-01],\n",
      "        [ 5.7209e-01, -4.8317e-01],\n",
      "        [ 1.1959e-01, -2.7199e-01],\n",
      "        [-1.8073e-01,  6.9682e-02],\n",
      "        [ 3.1848e-01, -2.9595e-01],\n",
      "        [ 4.4211e-01, -4.6590e-01],\n",
      "        [ 5.4242e-01, -3.3825e-01],\n",
      "        [ 4.4658e-01, -2.4294e-01],\n",
      "        [ 1.9420e-01, -1.0996e-01],\n",
      "        [ 3.7010e-01, -3.1587e-01],\n",
      "        [ 3.5584e-01, -2.1892e-01],\n",
      "        [ 2.4731e-01, -2.8037e-01],\n",
      "        [-2.2094e-01,  6.0078e-02],\n",
      "        [ 2.7191e-02, -2.9381e-02],\n",
      "        [ 3.4494e-01, -3.1766e-01],\n",
      "        [ 3.8149e-02, -8.5347e-03],\n",
      "        [ 9.0388e-01, -6.6583e-01],\n",
      "        [ 3.6433e-01, -2.5678e-01],\n",
      "        [ 3.3136e-02, -1.4407e-02],\n",
      "        [ 3.2722e-01, -2.5620e-01],\n",
      "        [ 6.4378e-01, -6.8626e-01],\n",
      "        [ 6.5823e-01, -8.6849e-01],\n",
      "        [-1.9111e-01,  1.5945e-01],\n",
      "        [-3.4378e-01,  2.2215e-01],\n",
      "        [ 6.9427e-01, -8.6622e-01],\n",
      "        [ 9.1020e-02, -4.1501e-01],\n",
      "        [ 9.7173e-02, -1.0147e-01],\n",
      "        [ 2.8018e-01, -2.3543e-01],\n",
      "        [ 8.4929e-02, -1.5265e-01],\n",
      "        [ 2.1170e-01, -2.0701e-01],\n",
      "        [ 1.7123e-01, -1.1348e-01],\n",
      "        [ 5.1810e-01, -4.3413e-01],\n",
      "        [-1.3477e-01, -5.2796e-02],\n",
      "        [-8.8722e-02,  1.3697e-01],\n",
      "        [ 2.5977e-01, -3.1085e-01],\n",
      "        [ 7.4448e-01, -6.0226e-01],\n",
      "        [ 2.1118e-01, -1.7370e-01],\n",
      "        [ 6.3393e-02,  1.4928e-02],\n",
      "        [ 7.7541e-01, -9.1263e-01],\n",
      "        [-3.0372e-01,  2.3410e-01],\n",
      "        [ 3.3570e-01, -5.8959e-01],\n",
      "        [ 6.7693e-01, -5.5304e-01],\n",
      "        [ 3.1443e-01, -4.0868e-01],\n",
      "        [-3.1765e-01,  2.2869e-01],\n",
      "        [ 8.8860e-02, -3.3927e-03],\n",
      "        [ 1.1958e+00, -1.4276e+00],\n",
      "        [-6.5935e-01,  3.8233e-01],\n",
      "        [ 5.4136e-01, -7.1113e-01],\n",
      "        [ 3.6225e-01, -3.5106e-01],\n",
      "        [-7.4272e-03,  1.0891e-01],\n",
      "        [ 4.8881e-01, -5.7640e-01],\n",
      "        [ 8.5094e-02, -2.9816e-02],\n",
      "        [ 4.9569e-01, -5.1807e-01],\n",
      "        [ 1.0686e-01, -2.6387e-02],\n",
      "        [ 2.2310e-01, -2.9437e-01],\n",
      "        [ 7.3186e-01, -7.8638e-01],\n",
      "        [-1.3763e-01,  1.4963e-01],\n",
      "        [ 6.2221e-01, -6.8178e-01],\n",
      "        [ 1.7380e-01, -2.5561e-01],\n",
      "        [ 2.5421e-01, -3.0339e-01],\n",
      "        [ 1.4193e-01, -5.2782e-02],\n",
      "        [-1.0634e-01,  1.3067e-03],\n",
      "        [ 5.8301e-01, -6.0310e-01],\n",
      "        [ 1.2285e-01, -1.6021e-01],\n",
      "        [ 3.0202e-01, -3.1097e-01],\n",
      "        [ 2.1939e-01, -1.5268e-01],\n",
      "        [-3.1704e-02,  2.9415e-02],\n",
      "        [ 2.9538e-01, -2.4587e-01],\n",
      "        [ 5.7788e-01, -7.1581e-01],\n",
      "        [-1.8367e-01,  2.6877e-01],\n",
      "        [ 2.2252e+00, -2.8933e+00],\n",
      "        [ 7.1375e-02, -1.8639e-01],\n",
      "        [ 1.5228e-03,  9.2603e-02],\n",
      "        [ 7.2763e-02, -1.1121e-01],\n",
      "        [-1.9646e-01,  2.1649e-01],\n",
      "        [ 4.0334e-01, -2.5014e-01],\n",
      "        [ 5.1415e-02, -9.6222e-03],\n",
      "        [ 5.6716e-01, -5.7617e-01],\n",
      "        [ 3.4356e-02,  9.1167e-02],\n",
      "        [ 6.9797e-01, -6.2856e-01],\n",
      "        [ 8.9074e-01, -1.0611e+00],\n",
      "        [ 1.7582e-01, -2.0184e-01],\n",
      "        [-9.5972e-02, -5.4886e-02],\n",
      "        [ 1.2899e-01, -1.0436e-01],\n",
      "        [ 2.4893e-01, -3.3572e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [1/149], Loss: 0.5891\n",
      "Epoch [6/10], Step [2/149], Loss: 0.6137\n",
      "Epoch [6/10], Step [3/149], Loss: 0.5982\n",
      "Epoch [6/10], Step [4/149], Loss: 0.5763\n",
      "Epoch [6/10], Step [5/149], Loss: 0.5255\n",
      "Epoch [6/10], Step [6/149], Loss: 0.6321\n",
      "Epoch [6/10], Step [7/149], Loss: 0.5763\n",
      "Epoch [6/10], Step [8/149], Loss: 0.6014\n",
      "Epoch [6/10], Step [9/149], Loss: 0.5695\n",
      "Epoch [6/10], Step [10/149], Loss: 0.5344\n",
      "Epoch [6/10], Step [11/149], Loss: 0.5394\n",
      "Epoch [6/10], Step [12/149], Loss: 0.5195\n",
      "Epoch [6/10], Step [13/149], Loss: 0.5906\n",
      "Epoch [6/10], Step [14/149], Loss: 0.5490\n",
      "Epoch [6/10], Step [15/149], Loss: 0.5404\n",
      "Epoch [6/10], Step [16/149], Loss: 0.5562\n",
      "Epoch [6/10], Step [17/149], Loss: 0.5358\n",
      "Epoch [6/10], Step [18/149], Loss: 0.5279\n",
      "Epoch [6/10], Step [19/149], Loss: 0.4992\n",
      "Epoch [6/10], Step [20/149], Loss: 0.5312\n",
      "Epoch [6/10], Step [21/149], Loss: 0.5227\n",
      "Epoch [6/10], Step [22/149], Loss: 0.5113\n",
      "Epoch [6/10], Step [23/149], Loss: 0.5359\n",
      "Epoch [6/10], Step [24/149], Loss: 0.5310\n",
      "Epoch [6/10], Step [25/149], Loss: 0.5297\n",
      "Epoch [6/10], Step [26/149], Loss: 0.5755\n",
      "Epoch [6/10], Step [27/149], Loss: 0.5129\n",
      "Epoch [6/10], Step [28/149], Loss: 0.4941\n",
      "Epoch [6/10], Step [29/149], Loss: 0.5106\n",
      "Epoch [6/10], Step [30/149], Loss: 0.5337\n",
      "Epoch [6/10], Step [31/149], Loss: 0.5507\n",
      "Epoch [6/10], Step [32/149], Loss: 0.5847\n",
      "Epoch [6/10], Step [33/149], Loss: 0.5617\n",
      "Epoch [6/10], Step [34/149], Loss: 0.6090\n",
      "Epoch [6/10], Step [35/149], Loss: 0.6247\n",
      "Epoch [6/10], Step [36/149], Loss: 0.4977\n",
      "Epoch [6/10], Step [37/149], Loss: 0.5705\n",
      "Epoch [6/10], Step [38/149], Loss: 0.5857\n",
      "Epoch [6/10], Step [39/149], Loss: 0.5120\n",
      "Epoch [6/10], Step [40/149], Loss: 0.5590\n",
      "Epoch [6/10], Step [41/149], Loss: 0.5219\n",
      "Epoch [6/10], Step [42/149], Loss: 0.5422\n",
      "Epoch [6/10], Step [43/149], Loss: 0.5331\n",
      "Epoch [6/10], Step [44/149], Loss: 0.5232\n",
      "Epoch [6/10], Step [45/149], Loss: 0.5903\n",
      "Epoch [6/10], Step [46/149], Loss: 0.5503\n",
      "Epoch [6/10], Step [47/149], Loss: 0.5630\n",
      "Epoch [6/10], Step [48/149], Loss: 0.5684\n",
      "Epoch [6/10], Step [49/149], Loss: 0.5632\n",
      "Epoch [6/10], Step [50/149], Loss: 0.5264\n",
      "Epoch [6/10], Step [51/149], Loss: 0.5301\n",
      "Epoch [6/10], Step [52/149], Loss: 0.5875\n",
      "Epoch [6/10], Step [53/149], Loss: 0.5348\n",
      "Epoch [6/10], Step [54/149], Loss: 0.5625\n",
      "Epoch [6/10], Step [55/149], Loss: 0.5686\n",
      "Epoch [6/10], Step [56/149], Loss: 0.4773\n",
      "Epoch [6/10], Step [57/149], Loss: 0.5492\n",
      "Epoch [6/10], Step [58/149], Loss: 0.5249\n",
      "Epoch [6/10], Step [59/149], Loss: 0.5374\n",
      "Epoch [6/10], Step [60/149], Loss: 0.5774\n",
      "Epoch [6/10], Step [61/149], Loss: 0.5940\n",
      "Epoch [6/10], Step [62/149], Loss: 0.5426\n",
      "Epoch [6/10], Step [63/149], Loss: 0.4929\n",
      "Epoch [6/10], Step [64/149], Loss: 0.5363\n",
      "Epoch [6/10], Step [65/149], Loss: 0.5167\n",
      "Epoch [6/10], Step [66/149], Loss: 0.5269\n",
      "Epoch [6/10], Step [67/149], Loss: 0.4573\n",
      "Epoch [6/10], Step [68/149], Loss: 0.5398\n",
      "Epoch [6/10], Step [69/149], Loss: 0.5991\n",
      "Epoch [6/10], Step [70/149], Loss: 0.5801\n",
      "Epoch [6/10], Step [71/149], Loss: 0.5594\n",
      "Epoch [6/10], Step [72/149], Loss: 0.5450\n",
      "Epoch [6/10], Step [73/149], Loss: 0.5486\n",
      "Epoch [6/10], Step [74/149], Loss: 0.5218\n",
      "Epoch [6/10], Step [75/149], Loss: 0.5124\n",
      "Epoch [6/10], Step [76/149], Loss: 0.4898\n",
      "Epoch [6/10], Step [77/149], Loss: 0.5217\n",
      "Epoch [6/10], Step [78/149], Loss: 0.5353\n",
      "Epoch [6/10], Step [79/149], Loss: 0.5234\n",
      "Epoch [6/10], Step [80/149], Loss: 0.5699\n",
      "Epoch [6/10], Step [81/149], Loss: 0.5938\n",
      "Epoch [6/10], Step [82/149], Loss: 0.5226\n",
      "Epoch [6/10], Step [83/149], Loss: 0.5031\n",
      "Epoch [6/10], Step [84/149], Loss: 0.5187\n",
      "Epoch [6/10], Step [85/149], Loss: 0.5645\n",
      "Epoch [6/10], Step [86/149], Loss: 0.6055\n",
      "Epoch [6/10], Step [87/149], Loss: 0.5277\n",
      "Epoch [6/10], Step [88/149], Loss: 0.4304\n",
      "Epoch [6/10], Step [89/149], Loss: 0.5079\n",
      "Epoch [6/10], Step [90/149], Loss: 0.4988\n",
      "Epoch [6/10], Step [91/149], Loss: 0.5874\n",
      "Epoch [6/10], Step [92/149], Loss: 0.5304\n",
      "Epoch [6/10], Step [93/149], Loss: 0.5067\n",
      "Epoch [6/10], Step [94/149], Loss: 0.5123\n",
      "Epoch [6/10], Step [95/149], Loss: 0.5024\n",
      "Epoch [6/10], Step [96/149], Loss: 0.5043\n",
      "Epoch [6/10], Step [97/149], Loss: 0.5054\n",
      "Epoch [6/10], Step [98/149], Loss: 0.5567\n",
      "Epoch [6/10], Step [99/149], Loss: 0.6226\n",
      "Epoch [6/10], Step [100/149], Loss: 0.4722\n",
      "Epoch [6/10], Step [101/149], Loss: 0.5920\n",
      "Epoch [6/10], Step [102/149], Loss: 0.4990\n",
      "Epoch [6/10], Step [103/149], Loss: 0.5495\n",
      "Epoch [6/10], Step [104/149], Loss: 0.5332\n",
      "Epoch [6/10], Step [105/149], Loss: 0.5625\n",
      "Epoch [6/10], Step [106/149], Loss: 0.5388\n",
      "Epoch [6/10], Step [107/149], Loss: 0.4705\n",
      "Epoch [6/10], Step [108/149], Loss: 0.4837\n",
      "Epoch [6/10], Step [109/149], Loss: 0.5143\n",
      "Epoch [6/10], Step [110/149], Loss: 0.5004\n",
      "Epoch [6/10], Step [111/149], Loss: 0.5251\n",
      "Epoch [6/10], Step [112/149], Loss: 0.5875\n",
      "Epoch [6/10], Step [113/149], Loss: 0.5007\n",
      "Epoch [6/10], Step [114/149], Loss: 0.5288\n",
      "Epoch [6/10], Step [115/149], Loss: 0.5105\n",
      "Epoch [6/10], Step [116/149], Loss: 0.4698\n",
      "Epoch [6/10], Step [117/149], Loss: 0.5343\n",
      "Epoch [6/10], Step [118/149], Loss: 0.5155\n",
      "Epoch [6/10], Step [119/149], Loss: 0.5946\n",
      "Epoch [6/10], Step [120/149], Loss: 0.6257\n",
      "Epoch [6/10], Step [121/149], Loss: 0.5437\n",
      "Epoch [6/10], Step [122/149], Loss: 0.4945\n",
      "Epoch [6/10], Step [123/149], Loss: 0.5620\n",
      "Epoch [6/10], Step [124/149], Loss: 0.5138\n",
      "Epoch [6/10], Step [125/149], Loss: 0.4755\n",
      "Epoch [6/10], Step [126/149], Loss: 0.5693\n",
      "Epoch [6/10], Step [127/149], Loss: 0.5603\n",
      "Epoch [6/10], Step [128/149], Loss: 0.5215\n",
      "Epoch [6/10], Step [129/149], Loss: 0.5022\n",
      "Epoch [6/10], Step [130/149], Loss: 0.5887\n",
      "Epoch [6/10], Step [131/149], Loss: 0.5553\n",
      "Epoch [6/10], Step [132/149], Loss: 0.5040\n",
      "Epoch [6/10], Step [133/149], Loss: 0.6065\n",
      "Epoch [6/10], Step [134/149], Loss: 0.5212\n",
      "Epoch [6/10], Step [135/149], Loss: 0.5658\n",
      "Epoch [6/10], Step [136/149], Loss: 0.5624\n",
      "Epoch [6/10], Step [137/149], Loss: 0.4876\n",
      "Epoch [6/10], Step [138/149], Loss: 0.5031\n",
      "Epoch [6/10], Step [139/149], Loss: 0.5112\n",
      "Epoch [6/10], Step [140/149], Loss: 0.4978\n",
      "Epoch [6/10], Step [141/149], Loss: 0.4929\n",
      "Epoch [6/10], Step [142/149], Loss: 0.5033\n",
      "Epoch [6/10], Step [143/149], Loss: 0.5299\n",
      "Epoch [6/10], Step [144/149], Loss: 0.4871\n",
      "Epoch [6/10], Step [145/149], Loss: 0.5630\n",
      "Epoch [6/10], Step [146/149], Loss: 0.5286\n",
      "Epoch [6/10], Step [147/149], Loss: 0.5823\n",
      "Epoch [6/10], Step [148/149], Loss: 0.5851\n",
      "Epoch [6/10], Step [149/149], Loss: 0.6158\n",
      "Epoch [6/10] and the best match:\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1])\n",
      "tensor([[-2.2532e-01,  2.0648e-01],\n",
      "        [ 1.2524e+00, -1.8090e+00],\n",
      "        [ 2.7018e-01, -2.4252e-01],\n",
      "        [-3.2846e-01,  1.4871e-01],\n",
      "        [ 3.5480e-01, -3.3305e-01],\n",
      "        [-4.7231e-01,  3.0346e-01],\n",
      "        [-1.2108e+00,  8.4566e-01],\n",
      "        [-1.6767e+00,  9.4416e-01],\n",
      "        [ 4.1849e-01, -3.9867e-01],\n",
      "        [-1.1990e+00,  6.8444e-01],\n",
      "        [-8.5590e-01,  6.1071e-01],\n",
      "        [ 2.6107e-01, -4.4821e-01],\n",
      "        [ 9.9945e-01, -1.2614e+00],\n",
      "        [-1.2326e-02, -2.6846e-01],\n",
      "        [-1.8249e-01,  2.4259e-01],\n",
      "        [ 3.1931e-01, -4.4580e-02],\n",
      "        [ 5.1527e-01, -3.1948e-01],\n",
      "        [-7.6325e-01,  6.6218e-01],\n",
      "        [-6.1812e-01,  3.7192e-01],\n",
      "        [-5.3446e-01,  3.3388e-01],\n",
      "        [ 9.9368e-01, -1.2302e+00],\n",
      "        [-9.2141e-02,  1.1727e-01],\n",
      "        [ 2.8672e-01, -2.5796e-01],\n",
      "        [-3.4750e-01,  2.7187e-01],\n",
      "        [-1.0338e+00,  5.2493e-01],\n",
      "        [ 1.2533e+00, -1.4672e+00],\n",
      "        [-3.6706e-01,  3.5711e-01],\n",
      "        [ 6.2797e-01, -6.1241e-01],\n",
      "        [-1.5593e+00,  1.1410e+00],\n",
      "        [ 6.7466e-01, -6.5361e-01],\n",
      "        [-5.8369e-01,  3.7974e-01],\n",
      "        [-1.3198e+00,  7.8487e-01],\n",
      "        [-6.6010e-01,  3.4023e-01],\n",
      "        [-6.4826e-02,  4.3248e-02],\n",
      "        [ 1.0536e+00, -1.1780e+00],\n",
      "        [ 3.3421e-01, -6.2004e-01],\n",
      "        [-8.6371e-01,  5.6783e-01],\n",
      "        [-3.6498e-01,  2.3474e-01],\n",
      "        [ 6.0303e-01, -7.5769e-01],\n",
      "        [ 4.5608e-01, -4.6649e-01],\n",
      "        [-2.8952e-01,  2.7599e-01],\n",
      "        [-8.0097e-02,  6.7790e-02],\n",
      "        [-8.8630e-01,  5.1812e-01],\n",
      "        [-2.6061e-01,  2.2750e-02],\n",
      "        [ 8.6618e-01, -4.9692e-01],\n",
      "        [-4.9260e-02,  8.4097e-02],\n",
      "        [ 5.4491e-01, -5.9993e-01],\n",
      "        [ 1.3401e-01, -9.9223e-02],\n",
      "        [-1.0459e-01,  1.2381e-01],\n",
      "        [ 8.8108e-01, -9.9037e-01],\n",
      "        [-4.5550e-01,  2.2100e-01],\n",
      "        [ 5.9234e-02, -9.9177e-02],\n",
      "        [-5.2679e-01,  2.4549e-01],\n",
      "        [-2.4861e-01, -4.6219e-02],\n",
      "        [-5.7361e-02,  3.0225e-02],\n",
      "        [-6.5975e-01,  5.3413e-01],\n",
      "        [-1.2994e-02,  1.2912e-01],\n",
      "        [ 5.2456e-01, -3.8034e-01],\n",
      "        [ 8.5646e-01, -6.6450e-01],\n",
      "        [-2.9992e-01,  3.0436e-01],\n",
      "        [ 5.0672e-01, -3.2654e-01],\n",
      "        [ 1.4810e-01, -1.9215e-01],\n",
      "        [ 6.6621e-01, -7.5435e-01],\n",
      "        [ 1.3669e+00, -1.2328e+00],\n",
      "        [ 3.2767e-01, -4.5174e-01],\n",
      "        [ 1.2462e+00, -1.2740e+00],\n",
      "        [ 9.7989e-01, -1.3563e+00],\n",
      "        [-1.8770e+00,  1.0763e+00],\n",
      "        [-1.8861e+00,  1.2976e+00],\n",
      "        [-8.1060e-01,  5.2616e-01],\n",
      "        [ 1.2836e-01, -2.3651e-01],\n",
      "        [ 8.4093e-01, -9.5328e-01],\n",
      "        [ 4.2952e-01, -7.8259e-01],\n",
      "        [ 4.0052e-01, -2.8343e-01],\n",
      "        [-1.3508e+00,  6.7107e-01],\n",
      "        [ 2.4943e-01, -3.4916e-01],\n",
      "        [ 9.8771e-01, -1.6673e+00],\n",
      "        [-3.3875e-01,  4.3856e-02],\n",
      "        [-5.0275e-01,  3.9528e-01],\n",
      "        [-5.3853e-01,  4.5346e-01],\n",
      "        [-1.6786e+00,  1.1749e+00],\n",
      "        [ 6.2939e-01, -7.4509e-01],\n",
      "        [ 2.6333e-01, -2.6607e-01],\n",
      "        [ 4.3357e-01, -3.4374e-01],\n",
      "        [-1.8974e-02, -3.4581e-02],\n",
      "        [ 6.1065e-01, -8.1855e-01],\n",
      "        [ 1.1471e+00, -1.2026e+00],\n",
      "        [ 9.3468e-02, -1.3902e-03],\n",
      "        [-8.0540e-01,  3.3085e-01],\n",
      "        [-4.8262e-01,  3.4200e-01],\n",
      "        [-8.0299e-01,  6.5037e-01],\n",
      "        [-3.1156e-01,  6.1939e-02],\n",
      "        [ 4.9458e-02, -3.8414e-01],\n",
      "        [-6.3358e-01,  5.6854e-01],\n",
      "        [ 4.8732e-01, -6.9113e-01],\n",
      "        [ 9.4708e-01, -1.0378e+00],\n",
      "        [-4.9302e-01,  3.8117e-01],\n",
      "        [ 8.3948e-01, -6.4610e-01],\n",
      "        [-3.7144e-01,  2.7305e-01],\n",
      "        [ 5.8207e-01, -9.1592e-01],\n",
      "        [ 9.6286e-01, -7.9308e-01],\n",
      "        [ 8.7044e-01, -9.4780e-01],\n",
      "        [-7.5750e-01,  5.1464e-01],\n",
      "        [ 1.7101e+00, -2.0211e+00],\n",
      "        [-1.4085e+00,  7.7286e-01],\n",
      "        [-7.9210e-02,  9.1686e-02],\n",
      "        [-3.1097e-01, -1.7765e-03],\n",
      "        [ 1.0468e+00, -1.0155e+00],\n",
      "        [ 3.1088e-01, -5.1448e-01],\n",
      "        [ 7.8771e-02,  1.6130e-02],\n",
      "        [ 6.0555e-01, -6.6045e-01],\n",
      "        [ 1.0594e+00, -1.1258e+00],\n",
      "        [ 4.8598e-01, -4.7392e-01],\n",
      "        [ 2.7354e-01, -3.5496e-01],\n",
      "        [ 2.4125e-01, -3.3630e-01],\n",
      "        [ 8.2519e-01, -9.3732e-01],\n",
      "        [-3.4416e-01,  2.2435e-01],\n",
      "        [ 1.8979e+00, -1.9348e+00],\n",
      "        [-1.2015e+00,  8.8056e-01],\n",
      "        [-1.2094e-01,  2.1727e-01],\n",
      "        [-5.8705e-01,  5.2655e-01],\n",
      "        [-1.0676e+00,  1.1099e+00],\n",
      "        [ 2.0821e-01, -1.0174e-01],\n",
      "        [-1.6074e-02, -2.7644e-01],\n",
      "        [ 1.2821e+00, -1.5034e+00],\n",
      "        [ 2.9446e-01, -2.5079e-01],\n",
      "        [-6.6572e-01,  2.9591e-01],\n",
      "        [-1.1723e+00,  1.0334e+00]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [1/149], Loss: 0.4738\n",
      "Epoch [7/10], Step [2/149], Loss: 0.4703\n",
      "Epoch [7/10], Step [3/149], Loss: 0.4557\n",
      "Epoch [7/10], Step [4/149], Loss: 0.4750\n",
      "Epoch [7/10], Step [5/149], Loss: 0.3891\n",
      "Epoch [7/10], Step [6/149], Loss: 0.4421\n",
      "Epoch [7/10], Step [7/149], Loss: 0.5022\n",
      "Epoch [7/10], Step [8/149], Loss: 0.4054\n",
      "Epoch [7/10], Step [9/149], Loss: 0.3835\n",
      "Epoch [7/10], Step [10/149], Loss: 0.4547\n",
      "Epoch [7/10], Step [11/149], Loss: 0.4045\n",
      "Epoch [7/10], Step [12/149], Loss: 0.4256\n",
      "Epoch [7/10], Step [13/149], Loss: 0.3961\n",
      "Epoch [7/10], Step [14/149], Loss: 0.4229\n",
      "Epoch [7/10], Step [15/149], Loss: 0.4258\n",
      "Epoch [7/10], Step [16/149], Loss: 0.4225\n",
      "Epoch [7/10], Step [17/149], Loss: 0.4194\n",
      "Epoch [7/10], Step [18/149], Loss: 0.4319\n",
      "Epoch [7/10], Step [19/149], Loss: 0.4380\n",
      "Epoch [7/10], Step [20/149], Loss: 0.4915\n",
      "Epoch [7/10], Step [21/149], Loss: 0.4460\n",
      "Epoch [7/10], Step [22/149], Loss: 0.4645\n",
      "Epoch [7/10], Step [23/149], Loss: 0.4914\n",
      "Epoch [7/10], Step [24/149], Loss: 0.3864\n",
      "Epoch [7/10], Step [25/149], Loss: 0.4710\n",
      "Epoch [7/10], Step [26/149], Loss: 0.4573\n",
      "Epoch [7/10], Step [27/149], Loss: 0.3555\n",
      "Epoch [7/10], Step [28/149], Loss: 0.3753\n",
      "Epoch [7/10], Step [29/149], Loss: 0.4446\n",
      "Epoch [7/10], Step [30/149], Loss: 0.3754\n",
      "Epoch [7/10], Step [31/149], Loss: 0.4817\n",
      "Epoch [7/10], Step [32/149], Loss: 0.4699\n",
      "Epoch [7/10], Step [33/149], Loss: 0.4937\n",
      "Epoch [7/10], Step [34/149], Loss: 0.4335\n",
      "Epoch [7/10], Step [35/149], Loss: 0.4006\n",
      "Epoch [7/10], Step [36/149], Loss: 0.4102\n",
      "Epoch [7/10], Step [37/149], Loss: 0.4507\n",
      "Epoch [7/10], Step [38/149], Loss: 0.5092\n",
      "Epoch [7/10], Step [39/149], Loss: 0.4669\n",
      "Epoch [7/10], Step [40/149], Loss: 0.4676\n",
      "Epoch [7/10], Step [41/149], Loss: 0.3968\n",
      "Epoch [7/10], Step [42/149], Loss: 0.4230\n",
      "Epoch [7/10], Step [43/149], Loss: 0.4685\n",
      "Epoch [7/10], Step [44/149], Loss: 0.4453\n",
      "Epoch [7/10], Step [45/149], Loss: 0.4586\n",
      "Epoch [7/10], Step [46/149], Loss: 0.4515\n",
      "Epoch [7/10], Step [47/149], Loss: 0.4636\n",
      "Epoch [7/10], Step [48/149], Loss: 0.5063\n",
      "Epoch [7/10], Step [49/149], Loss: 0.4026\n",
      "Epoch [7/10], Step [50/149], Loss: 0.4354\n",
      "Epoch [7/10], Step [51/149], Loss: 0.4997\n",
      "Epoch [7/10], Step [52/149], Loss: 0.4541\n",
      "Epoch [7/10], Step [53/149], Loss: 0.4212\n",
      "Epoch [7/10], Step [54/149], Loss: 0.3917\n",
      "Epoch [7/10], Step [55/149], Loss: 0.4521\n",
      "Epoch [7/10], Step [56/149], Loss: 0.5085\n",
      "Epoch [7/10], Step [57/149], Loss: 0.4351\n",
      "Epoch [7/10], Step [58/149], Loss: 0.3795\n",
      "Epoch [7/10], Step [59/149], Loss: 0.4122\n",
      "Epoch [7/10], Step [60/149], Loss: 0.4203\n",
      "Epoch [7/10], Step [61/149], Loss: 0.4819\n",
      "Epoch [7/10], Step [62/149], Loss: 0.4499\n",
      "Epoch [7/10], Step [63/149], Loss: 0.4507\n",
      "Epoch [7/10], Step [64/149], Loss: 0.4733\n",
      "Epoch [7/10], Step [65/149], Loss: 0.4955\n",
      "Epoch [7/10], Step [66/149], Loss: 0.5004\n",
      "Epoch [7/10], Step [67/149], Loss: 0.4655\n",
      "Epoch [7/10], Step [68/149], Loss: 0.4923\n",
      "Epoch [7/10], Step [69/149], Loss: 0.4041\n",
      "Epoch [7/10], Step [70/149], Loss: 0.4434\n",
      "Epoch [7/10], Step [71/149], Loss: 0.4294\n",
      "Epoch [7/10], Step [72/149], Loss: 0.4702\n",
      "Epoch [7/10], Step [73/149], Loss: 0.4467\n",
      "Epoch [7/10], Step [74/149], Loss: 0.4402\n",
      "Epoch [7/10], Step [75/149], Loss: 0.4049\n",
      "Epoch [7/10], Step [76/149], Loss: 0.4623\n",
      "Epoch [7/10], Step [77/149], Loss: 0.4539\n",
      "Epoch [7/10], Step [78/149], Loss: 0.5001\n",
      "Epoch [7/10], Step [79/149], Loss: 0.4800\n",
      "Epoch [7/10], Step [80/149], Loss: 0.4186\n",
      "Epoch [7/10], Step [81/149], Loss: 0.5365\n",
      "Epoch [7/10], Step [82/149], Loss: 0.4826\n",
      "Epoch [7/10], Step [83/149], Loss: 0.4900\n",
      "Epoch [7/10], Step [84/149], Loss: 0.4410\n",
      "Epoch [7/10], Step [85/149], Loss: 0.4797\n",
      "Epoch [7/10], Step [86/149], Loss: 0.3690\n",
      "Epoch [7/10], Step [87/149], Loss: 0.4228\n",
      "Epoch [7/10], Step [88/149], Loss: 0.4229\n",
      "Epoch [7/10], Step [89/149], Loss: 0.4584\n",
      "Epoch [7/10], Step [90/149], Loss: 0.5332\n",
      "Epoch [7/10], Step [91/149], Loss: 0.4302\n",
      "Epoch [7/10], Step [92/149], Loss: 0.4928\n",
      "Epoch [7/10], Step [93/149], Loss: 0.4407\n",
      "Epoch [7/10], Step [94/149], Loss: 0.4267\n",
      "Epoch [7/10], Step [95/149], Loss: 0.4034\n",
      "Epoch [7/10], Step [96/149], Loss: 0.5400\n",
      "Epoch [7/10], Step [97/149], Loss: 0.4566\n",
      "Epoch [7/10], Step [98/149], Loss: 0.5202\n",
      "Epoch [7/10], Step [99/149], Loss: 0.4308\n",
      "Epoch [7/10], Step [100/149], Loss: 0.5139\n",
      "Epoch [7/10], Step [101/149], Loss: 0.3619\n",
      "Epoch [7/10], Step [102/149], Loss: 0.5140\n",
      "Epoch [7/10], Step [103/149], Loss: 0.5017\n",
      "Epoch [7/10], Step [104/149], Loss: 0.4364\n",
      "Epoch [7/10], Step [105/149], Loss: 0.4326\n",
      "Epoch [7/10], Step [106/149], Loss: 0.4760\n",
      "Epoch [7/10], Step [107/149], Loss: 0.4266\n",
      "Epoch [7/10], Step [108/149], Loss: 0.4886\n",
      "Epoch [7/10], Step [109/149], Loss: 0.4542\n",
      "Epoch [7/10], Step [110/149], Loss: 0.4816\n",
      "Epoch [7/10], Step [111/149], Loss: 0.4251\n",
      "Epoch [7/10], Step [112/149], Loss: 0.4049\n",
      "Epoch [7/10], Step [113/149], Loss: 0.4454\n",
      "Epoch [7/10], Step [114/149], Loss: 0.4617\n",
      "Epoch [7/10], Step [115/149], Loss: 0.4374\n",
      "Epoch [7/10], Step [116/149], Loss: 0.4418\n",
      "Epoch [7/10], Step [117/149], Loss: 0.4942\n",
      "Epoch [7/10], Step [118/149], Loss: 0.3830\n",
      "Epoch [7/10], Step [119/149], Loss: 0.4780\n",
      "Epoch [7/10], Step [120/149], Loss: 0.4335\n",
      "Epoch [7/10], Step [121/149], Loss: 0.4591\n",
      "Epoch [7/10], Step [122/149], Loss: 0.4558\n",
      "Epoch [7/10], Step [123/149], Loss: 0.4240\n",
      "Epoch [7/10], Step [124/149], Loss: 0.4654\n",
      "Epoch [7/10], Step [125/149], Loss: 0.4870\n",
      "Epoch [7/10], Step [126/149], Loss: 0.4629\n",
      "Epoch [7/10], Step [127/149], Loss: 0.4339\n",
      "Epoch [7/10], Step [128/149], Loss: 0.3904\n",
      "Epoch [7/10], Step [129/149], Loss: 0.4848\n",
      "Epoch [7/10], Step [130/149], Loss: 0.4235\n",
      "Epoch [7/10], Step [131/149], Loss: 0.4165\n",
      "Epoch [7/10], Step [132/149], Loss: 0.4142\n",
      "Epoch [7/10], Step [133/149], Loss: 0.4659\n",
      "Epoch [7/10], Step [134/149], Loss: 0.4044\n",
      "Epoch [7/10], Step [135/149], Loss: 0.4051\n",
      "Epoch [7/10], Step [136/149], Loss: 0.4622\n",
      "Epoch [7/10], Step [137/149], Loss: 0.3978\n",
      "Epoch [7/10], Step [138/149], Loss: 0.4199\n",
      "Epoch [7/10], Step [139/149], Loss: 0.3689\n",
      "Epoch [7/10], Step [140/149], Loss: 0.4380\n",
      "Epoch [7/10], Step [141/149], Loss: 0.3926\n",
      "Epoch [7/10], Step [142/149], Loss: 0.4917\n",
      "Epoch [7/10], Step [143/149], Loss: 0.4626\n",
      "Epoch [7/10], Step [144/149], Loss: 0.4355\n",
      "Epoch [7/10], Step [145/149], Loss: 0.4413\n",
      "Epoch [7/10], Step [146/149], Loss: 0.4199\n",
      "Epoch [7/10], Step [147/149], Loss: 0.4524\n",
      "Epoch [7/10], Step [148/149], Loss: 0.5335\n",
      "Epoch [7/10], Step [149/149], Loss: 0.3220\n",
      "Epoch [7/10] and the best match:\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
      "tensor([[-0.1707,  0.0462],\n",
      "        [-0.3253,  0.1222],\n",
      "        [-0.7777,  0.5727],\n",
      "        [ 0.5728, -0.7223],\n",
      "        [ 0.1665, -0.6183],\n",
      "        [ 0.1896, -0.3064],\n",
      "        [ 0.8906, -1.0599],\n",
      "        [-0.5149,  0.3110],\n",
      "        [ 2.0375, -2.0696],\n",
      "        [-1.2165,  0.8721],\n",
      "        [-1.0030,  0.5270],\n",
      "        [ 1.3844, -1.8479],\n",
      "        [ 1.6463, -1.9045],\n",
      "        [ 1.7539, -2.0059],\n",
      "        [ 0.2000, -0.3858],\n",
      "        [-0.9291,  0.7391],\n",
      "        [-1.2060,  0.9067],\n",
      "        [-0.3772,  0.3969],\n",
      "        [-1.3982,  0.7854],\n",
      "        [-0.8847,  0.8872]], grad_fn=<AddmmBackward0>)\n",
      "Epoch [8/10], Step [1/149], Loss: 0.3803\n",
      "Epoch [8/10], Step [2/149], Loss: 0.3360\n",
      "Epoch [8/10], Step [3/149], Loss: 0.3101\n",
      "Epoch [8/10], Step [4/149], Loss: 0.3212\n",
      "Epoch [8/10], Step [5/149], Loss: 0.3289\n",
      "Epoch [8/10], Step [6/149], Loss: 0.3118\n",
      "Epoch [8/10], Step [7/149], Loss: 0.3217\n",
      "Epoch [8/10], Step [8/149], Loss: 0.3473\n",
      "Epoch [8/10], Step [9/149], Loss: 0.3762\n",
      "Epoch [8/10], Step [10/149], Loss: 0.4169\n",
      "Epoch [8/10], Step [11/149], Loss: 0.2596\n",
      "Epoch [8/10], Step [12/149], Loss: 0.2789\n",
      "Epoch [8/10], Step [13/149], Loss: 0.3548\n",
      "Epoch [8/10], Step [14/149], Loss: 0.3159\n",
      "Epoch [8/10], Step [15/149], Loss: 0.3674\n",
      "Epoch [8/10], Step [16/149], Loss: 0.3550\n",
      "Epoch [8/10], Step [17/149], Loss: 0.3086\n",
      "Epoch [8/10], Step [18/149], Loss: 0.3570\n",
      "Epoch [8/10], Step [19/149], Loss: 0.3084\n",
      "Epoch [8/10], Step [20/149], Loss: 0.3358\n",
      "Epoch [8/10], Step [21/149], Loss: 0.3429\n",
      "Epoch [8/10], Step [22/149], Loss: 0.3740\n",
      "Epoch [8/10], Step [23/149], Loss: 0.3285\n",
      "Epoch [8/10], Step [24/149], Loss: 0.3479\n",
      "Epoch [8/10], Step [25/149], Loss: 0.2677\n",
      "Epoch [8/10], Step [26/149], Loss: 0.3791\n",
      "Epoch [8/10], Step [27/149], Loss: 0.3934\n",
      "Epoch [8/10], Step [28/149], Loss: 0.3530\n",
      "Epoch [8/10], Step [29/149], Loss: 0.3598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [30/149], Loss: 0.3139\n",
      "Epoch [8/10], Step [31/149], Loss: 0.3537\n",
      "Epoch [8/10], Step [32/149], Loss: 0.4044\n",
      "Epoch [8/10], Step [33/149], Loss: 0.3441\n",
      "Epoch [8/10], Step [34/149], Loss: 0.3669\n",
      "Epoch [8/10], Step [35/149], Loss: 0.2865\n",
      "Epoch [8/10], Step [36/149], Loss: 0.3530\n",
      "Epoch [8/10], Step [37/149], Loss: 0.3563\n",
      "Epoch [8/10], Step [38/149], Loss: 0.2875\n",
      "Epoch [8/10], Step [39/149], Loss: 0.2960\n",
      "Epoch [8/10], Step [40/149], Loss: 0.3222\n",
      "Epoch [8/10], Step [41/149], Loss: 0.3701\n",
      "Epoch [8/10], Step [42/149], Loss: 0.3194\n",
      "Epoch [8/10], Step [43/149], Loss: 0.2879\n",
      "Epoch [8/10], Step [44/149], Loss: 0.3620\n",
      "Epoch [8/10], Step [45/149], Loss: 0.3753\n",
      "Epoch [8/10], Step [46/149], Loss: 0.3239\n",
      "Epoch [8/10], Step [47/149], Loss: 0.3420\n",
      "Epoch [8/10], Step [48/149], Loss: 0.3444\n",
      "Epoch [8/10], Step [49/149], Loss: 0.3135\n",
      "Epoch [8/10], Step [50/149], Loss: 0.3395\n",
      "Epoch [8/10], Step [51/149], Loss: 0.3710\n",
      "Epoch [8/10], Step [52/149], Loss: 0.4477\n",
      "Epoch [8/10], Step [53/149], Loss: 0.3584\n",
      "Epoch [8/10], Step [54/149], Loss: 0.3181\n",
      "Epoch [8/10], Step [55/149], Loss: 0.4337\n",
      "Epoch [8/10], Step [56/149], Loss: 0.4406\n",
      "Epoch [8/10], Step [57/149], Loss: 0.3376\n",
      "Epoch [8/10], Step [58/149], Loss: 0.3108\n",
      "Epoch [8/10], Step [59/149], Loss: 0.3617\n",
      "Epoch [8/10], Step [60/149], Loss: 0.3870\n",
      "Epoch [8/10], Step [61/149], Loss: 0.3815\n",
      "Epoch [8/10], Step [62/149], Loss: 0.3709\n",
      "Epoch [8/10], Step [63/149], Loss: 0.3450\n",
      "Epoch [8/10], Step [64/149], Loss: 0.3303\n",
      "Epoch [8/10], Step [65/149], Loss: 0.3926\n",
      "Epoch [8/10], Step [66/149], Loss: 0.3281\n",
      "Epoch [8/10], Step [67/149], Loss: 0.4049\n",
      "Epoch [8/10], Step [68/149], Loss: 0.3618\n",
      "Epoch [8/10], Step [69/149], Loss: 0.3429\n",
      "Epoch [8/10], Step [70/149], Loss: 0.3809\n",
      "Epoch [8/10], Step [71/149], Loss: 0.3758\n",
      "Epoch [8/10], Step [72/149], Loss: 0.3224\n",
      "Epoch [8/10], Step [73/149], Loss: 0.3619\n",
      "Epoch [8/10], Step [74/149], Loss: 0.3101\n",
      "Epoch [8/10], Step [75/149], Loss: 0.3439\n",
      "Epoch [8/10], Step [76/149], Loss: 0.2911\n",
      "Epoch [8/10], Step [77/149], Loss: 0.3362\n",
      "Epoch [8/10], Step [78/149], Loss: 0.3812\n",
      "Epoch [8/10], Step [79/149], Loss: 0.4059\n",
      "Epoch [8/10], Step [80/149], Loss: 0.3048\n",
      "Epoch [8/10], Step [81/149], Loss: 0.2673\n",
      "Epoch [8/10], Step [82/149], Loss: 0.3585\n",
      "Epoch [8/10], Step [83/149], Loss: 0.3745\n",
      "Epoch [8/10], Step [84/149], Loss: 0.3481\n",
      "Epoch [8/10], Step [85/149], Loss: 0.3104\n",
      "Epoch [8/10], Step [86/149], Loss: 0.3486\n",
      "Epoch [8/10], Step [87/149], Loss: 0.4112\n",
      "Epoch [8/10], Step [88/149], Loss: 0.4310\n",
      "Epoch [8/10], Step [89/149], Loss: 0.3287\n",
      "Epoch [8/10], Step [90/149], Loss: 0.3578\n",
      "Epoch [8/10], Step [91/149], Loss: 0.2814\n",
      "Epoch [8/10], Step [92/149], Loss: 0.3342\n",
      "Epoch [8/10], Step [93/149], Loss: 0.3529\n",
      "Epoch [8/10], Step [94/149], Loss: 0.4184\n",
      "Epoch [8/10], Step [95/149], Loss: 0.3751\n",
      "Epoch [8/10], Step [96/149], Loss: 0.3024\n",
      "Epoch [8/10], Step [97/149], Loss: 0.3507\n",
      "Epoch [8/10], Step [98/149], Loss: 0.4080\n",
      "Epoch [8/10], Step [99/149], Loss: 0.3507\n",
      "Epoch [8/10], Step [100/149], Loss: 0.2938\n",
      "Epoch [8/10], Step [101/149], Loss: 0.3487\n",
      "Epoch [8/10], Step [102/149], Loss: 0.3197\n",
      "Epoch [8/10], Step [103/149], Loss: 0.2880\n",
      "Epoch [8/10], Step [104/149], Loss: 0.3548\n",
      "Epoch [8/10], Step [105/149], Loss: 0.3311\n",
      "Epoch [8/10], Step [106/149], Loss: 0.3540\n",
      "Epoch [8/10], Step [107/149], Loss: 0.3817\n",
      "Epoch [8/10], Step [108/149], Loss: 0.3300\n",
      "Epoch [8/10], Step [109/149], Loss: 0.3642\n",
      "Epoch [8/10], Step [110/149], Loss: 0.4056\n",
      "Epoch [8/10], Step [111/149], Loss: 0.3332\n",
      "Epoch [8/10], Step [112/149], Loss: 0.3156\n",
      "Epoch [8/10], Step [113/149], Loss: 0.3225\n",
      "Epoch [8/10], Step [114/149], Loss: 0.4212\n",
      "Epoch [8/10], Step [115/149], Loss: 0.3332\n",
      "Epoch [8/10], Step [116/149], Loss: 0.2872\n",
      "Epoch [8/10], Step [117/149], Loss: 0.2877\n",
      "Epoch [8/10], Step [118/149], Loss: 0.3545\n",
      "Epoch [8/10], Step [119/149], Loss: 0.4331\n",
      "Epoch [8/10], Step [120/149], Loss: 0.5088\n",
      "Epoch [8/10], Step [121/149], Loss: 0.3886\n",
      "Epoch [8/10], Step [122/149], Loss: 0.3080\n",
      "Epoch [8/10], Step [123/149], Loss: 0.3467\n",
      "Epoch [8/10], Step [124/149], Loss: 0.3563\n",
      "Epoch [8/10], Step [125/149], Loss: 0.3558\n",
      "Epoch [8/10], Step [126/149], Loss: 0.3039\n",
      "Epoch [8/10], Step [127/149], Loss: 0.2977\n",
      "Epoch [8/10], Step [128/149], Loss: 0.4509\n",
      "Epoch [8/10], Step [129/149], Loss: 0.4316\n",
      "Epoch [8/10], Step [130/149], Loss: 0.3112\n",
      "Epoch [8/10], Step [131/149], Loss: 0.3272\n",
      "Epoch [8/10], Step [132/149], Loss: 0.3361\n",
      "Epoch [8/10], Step [133/149], Loss: 0.2867\n",
      "Epoch [8/10], Step [134/149], Loss: 0.4174\n",
      "Epoch [8/10], Step [135/149], Loss: 0.3585\n",
      "Epoch [8/10], Step [136/149], Loss: 0.3920\n",
      "Epoch [8/10], Step [137/149], Loss: 0.3031\n",
      "Epoch [8/10], Step [138/149], Loss: 0.3387\n",
      "Epoch [8/10], Step [139/149], Loss: 0.4125\n",
      "Epoch [8/10], Step [140/149], Loss: 0.3120\n",
      "Epoch [8/10], Step [141/149], Loss: 0.3738\n",
      "Epoch [8/10], Step [142/149], Loss: 0.3474\n",
      "Epoch [8/10], Step [143/149], Loss: 0.3503\n",
      "Epoch [8/10], Step [144/149], Loss: 0.3102\n",
      "Epoch [8/10], Step [145/149], Loss: 0.3090\n",
      "Epoch [8/10], Step [146/149], Loss: 0.4065\n",
      "Epoch [8/10], Step [147/149], Loss: 0.2956\n",
      "Epoch [8/10], Step [148/149], Loss: 0.3058\n",
      "Epoch [8/10], Step [149/149], Loss: 0.2666\n",
      "Epoch [8/10] and the best match:\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0])\n",
      "tensor([[-8.0987e-01,  4.3921e-01],\n",
      "        [-1.3919e-03, -8.9297e-02],\n",
      "        [-9.2723e-02, -8.8756e-02],\n",
      "        [ 1.2278e+00, -1.1908e+00],\n",
      "        [ 1.0697e+00, -1.0958e+00],\n",
      "        [-8.3076e-02, -2.8242e-01],\n",
      "        [ 1.8662e+00, -1.7478e+00],\n",
      "        [-5.2528e-01,  4.8456e-01],\n",
      "        [-3.4683e+00,  2.1265e+00],\n",
      "        [-1.9611e+00,  1.1352e+00],\n",
      "        [-5.8554e-01,  2.8043e-01],\n",
      "        [ 1.3812e+00, -1.7878e+00],\n",
      "        [ 5.4673e-01, -6.0116e-01],\n",
      "        [ 7.1972e-01, -1.2065e+00],\n",
      "        [ 1.2522e+00, -1.6065e+00],\n",
      "        [-1.1860e-01,  2.0468e-01],\n",
      "        [-3.2443e-01, -2.2520e-01],\n",
      "        [ 1.2537e+00, -1.0715e+00],\n",
      "        [ 3.9539e-01, -4.3717e-01],\n",
      "        [-1.1518e+00,  2.5627e-01],\n",
      "        [ 7.4803e-01, -1.0542e+00],\n",
      "        [-2.9237e+00,  1.3052e+00],\n",
      "        [ 5.0705e-01, -7.2740e-01],\n",
      "        [ 2.0918e-01, -1.6317e-01],\n",
      "        [-2.9763e-01, -2.9362e-01],\n",
      "        [ 2.1866e+00, -1.9968e+00],\n",
      "        [-3.2195e-01,  2.8605e-01],\n",
      "        [-9.1051e-01,  5.7747e-01],\n",
      "        [ 7.5999e-01, -7.9214e-01],\n",
      "        [ 1.4542e+00, -1.8014e+00],\n",
      "        [ 3.3230e+00, -4.6993e+00],\n",
      "        [ 6.2671e-01, -9.7317e-01],\n",
      "        [ 8.7961e-01, -1.0606e+00],\n",
      "        [ 1.4871e+00, -1.6477e+00],\n",
      "        [-6.3012e-01,  3.4826e-01],\n",
      "        [-1.5865e+00,  9.7022e-01],\n",
      "        [ 6.4911e-01, -6.9482e-01],\n",
      "        [ 1.5178e+00, -1.9619e+00],\n",
      "        [ 1.5621e-01, -3.6799e-02],\n",
      "        [ 1.1843e+00, -1.5247e+00],\n",
      "        [ 1.3717e-01, -3.1862e-01],\n",
      "        [ 9.4827e-01, -6.1846e-01],\n",
      "        [ 4.9813e-01, -7.6461e-01],\n",
      "        [ 1.0141e+00, -1.1765e+00],\n",
      "        [-1.1842e+00,  5.9830e-01],\n",
      "        [-8.5912e-01,  6.9427e-01],\n",
      "        [-9.3541e-01,  8.1389e-01],\n",
      "        [-1.9481e+00,  1.0331e+00],\n",
      "        [ 1.6704e+00, -1.1573e+00],\n",
      "        [-1.7767e+00,  1.1770e+00],\n",
      "        [ 1.5160e+00, -1.5466e+00],\n",
      "        [-4.0388e-03, -8.1452e-02],\n",
      "        [ 7.9078e-01, -7.6878e-01],\n",
      "        [ 5.4393e-01, -1.0743e+00],\n",
      "        [-5.2125e-01,  2.1897e-01],\n",
      "        [ 3.4247e-01, -1.5022e+00],\n",
      "        [ 1.5654e+00, -1.5301e+00],\n",
      "        [-7.2226e-01,  8.0320e-01],\n",
      "        [ 1.6465e+00, -1.6839e+00],\n",
      "        [ 4.4222e+00, -6.2535e+00],\n",
      "        [-6.1548e-01,  3.8039e-01],\n",
      "        [-2.6137e+00,  1.6888e+00],\n",
      "        [ 1.3003e-01, -1.6260e-01],\n",
      "        [ 2.1849e-01, -4.8454e-01],\n",
      "        [ 2.7882e+00, -2.3311e+00],\n",
      "        [-4.0420e-01,  4.1777e-01],\n",
      "        [ 5.9923e-01, -9.3142e-01],\n",
      "        [-8.7307e-02, -3.1328e-01],\n",
      "        [-7.2665e-01,  3.4548e-01],\n",
      "        [ 4.5891e-01, -3.8024e-01],\n",
      "        [ 4.2144e-02, -3.3513e-01],\n",
      "        [ 1.2382e+00, -1.1169e+00],\n",
      "        [ 2.7165e-01, -7.5695e-01],\n",
      "        [ 1.3859e+00, -1.3628e+00],\n",
      "        [-1.4068e+00,  7.6231e-01],\n",
      "        [-5.1268e-01,  3.8596e-01],\n",
      "        [-5.9268e-01,  3.9249e-01],\n",
      "        [ 2.7330e-01, -2.0644e-01],\n",
      "        [-5.7921e-01,  2.8390e-01],\n",
      "        [ 1.1687e+00, -8.4348e-01],\n",
      "        [-5.5054e-01,  1.5535e-01],\n",
      "        [-1.5019e+00,  8.7070e-01],\n",
      "        [ 1.0473e+00, -1.3793e+00],\n",
      "        [ 5.8690e-01, -7.7865e-01],\n",
      "        [-8.8494e-02, -4.1028e-02],\n",
      "        [ 1.4998e+00, -1.6584e+00],\n",
      "        [-8.0404e-01,  4.3772e-01],\n",
      "        [ 7.3543e-01, -1.5432e+00],\n",
      "        [-1.1530e+00,  8.3009e-01],\n",
      "        [ 2.2819e+00, -2.4456e+00],\n",
      "        [ 2.4349e+00, -2.5742e+00],\n",
      "        [-1.7387e+00,  1.0708e+00],\n",
      "        [ 1.0190e+00, -1.3709e+00],\n",
      "        [-4.2953e-01,  1.5091e-01],\n",
      "        [-9.1531e-01,  3.7779e-01],\n",
      "        [-1.5825e+00,  9.6219e-01],\n",
      "        [-1.0766e+00, -1.8602e-01],\n",
      "        [-5.5169e-01,  5.4800e-01],\n",
      "        [-1.2838e-01,  1.1259e-01],\n",
      "        [ 6.6628e-01, -8.9706e-01],\n",
      "        [ 5.5028e-01, -8.6165e-01],\n",
      "        [ 1.1950e+00, -1.2597e+00],\n",
      "        [-2.5370e+00,  1.6339e+00],\n",
      "        [ 2.3242e-01, -8.1662e-01],\n",
      "        [ 3.1307e+00, -3.1753e+00],\n",
      "        [ 9.3409e-01, -1.0622e+00],\n",
      "        [ 2.1120e+00, -2.8745e+00],\n",
      "        [ 3.1135e-01, -3.2707e-01],\n",
      "        [-1.3112e+00,  9.1027e-01],\n",
      "        [ 4.9880e-01, -1.2643e+00],\n",
      "        [ 9.4600e-01, -7.6597e-01],\n",
      "        [ 9.1131e-01, -9.1146e-01],\n",
      "        [-7.1947e-01,  6.1056e-01],\n",
      "        [ 1.0189e+00, -9.1289e-01],\n",
      "        [ 5.5341e-01, -7.8056e-01],\n",
      "        [-1.7043e+00,  1.0669e+00],\n",
      "        [ 2.6286e+00, -2.3658e+00],\n",
      "        [ 5.7108e-01, -1.1346e+00],\n",
      "        [ 1.2124e+00, -1.2613e+00],\n",
      "        [-5.0424e-01, -1.2151e-01],\n",
      "        [ 6.9857e-01, -8.1733e-01],\n",
      "        [ 3.5242e-01, -6.4451e-01],\n",
      "        [-3.1979e+00,  2.1111e+00],\n",
      "        [-9.8002e-01,  8.6388e-01],\n",
      "        [ 1.5595e+00, -2.1018e+00],\n",
      "        [ 3.6057e+00, -3.9056e+00],\n",
      "        [-1.4546e+00,  7.4772e-01],\n",
      "        [ 9.9728e-01, -1.0344e+00]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [1/149], Loss: 0.3465\n",
      "Epoch [9/10], Step [2/149], Loss: 0.2775\n",
      "Epoch [9/10], Step [3/149], Loss: 0.2670\n",
      "Epoch [9/10], Step [4/149], Loss: 0.2829\n",
      "Epoch [9/10], Step [5/149], Loss: 0.2889\n",
      "Epoch [9/10], Step [6/149], Loss: 0.1904\n",
      "Epoch [9/10], Step [7/149], Loss: 0.2858\n",
      "Epoch [9/10], Step [8/149], Loss: 0.2294\n",
      "Epoch [9/10], Step [9/149], Loss: 0.3189\n",
      "Epoch [9/10], Step [10/149], Loss: 0.2263\n",
      "Epoch [9/10], Step [11/149], Loss: 0.2718\n",
      "Epoch [9/10], Step [12/149], Loss: 0.2633\n",
      "Epoch [9/10], Step [13/149], Loss: 0.2040\n",
      "Epoch [9/10], Step [14/149], Loss: 0.3173\n",
      "Epoch [9/10], Step [15/149], Loss: 0.3308\n",
      "Epoch [9/10], Step [16/149], Loss: 0.3027\n",
      "Epoch [9/10], Step [17/149], Loss: 0.2508\n",
      "Epoch [9/10], Step [18/149], Loss: 0.2665\n",
      "Epoch [9/10], Step [19/149], Loss: 0.2304\n",
      "Epoch [9/10], Step [20/149], Loss: 0.2590\n",
      "Epoch [9/10], Step [21/149], Loss: 0.2796\n",
      "Epoch [9/10], Step [22/149], Loss: 0.2836\n",
      "Epoch [9/10], Step [23/149], Loss: 0.1968\n",
      "Epoch [9/10], Step [24/149], Loss: 0.2776\n",
      "Epoch [9/10], Step [25/149], Loss: 0.2294\n",
      "Epoch [9/10], Step [26/149], Loss: 0.2603\n",
      "Epoch [9/10], Step [27/149], Loss: 0.2223\n",
      "Epoch [9/10], Step [28/149], Loss: 0.2651\n",
      "Epoch [9/10], Step [29/149], Loss: 0.2252\n",
      "Epoch [9/10], Step [30/149], Loss: 0.2345\n",
      "Epoch [9/10], Step [31/149], Loss: 0.2752\n",
      "Epoch [9/10], Step [32/149], Loss: 0.3358\n",
      "Epoch [9/10], Step [33/149], Loss: 0.2206\n",
      "Epoch [9/10], Step [34/149], Loss: 0.2679\n",
      "Epoch [9/10], Step [35/149], Loss: 0.2395\n",
      "Epoch [9/10], Step [36/149], Loss: 0.2337\n",
      "Epoch [9/10], Step [37/149], Loss: 0.2344\n",
      "Epoch [9/10], Step [38/149], Loss: 0.2323\n",
      "Epoch [9/10], Step [39/149], Loss: 0.2715\n",
      "Epoch [9/10], Step [40/149], Loss: 0.2350\n",
      "Epoch [9/10], Step [41/149], Loss: 0.2618\n",
      "Epoch [9/10], Step [42/149], Loss: 0.2513\n",
      "Epoch [9/10], Step [43/149], Loss: 0.2338\n",
      "Epoch [9/10], Step [44/149], Loss: 0.2784\n",
      "Epoch [9/10], Step [45/149], Loss: 0.2537\n",
      "Epoch [9/10], Step [46/149], Loss: 0.2246\n",
      "Epoch [9/10], Step [47/149], Loss: 0.2800\n",
      "Epoch [9/10], Step [48/149], Loss: 0.2353\n",
      "Epoch [9/10], Step [49/149], Loss: 0.2973\n",
      "Epoch [9/10], Step [50/149], Loss: 0.2229\n",
      "Epoch [9/10], Step [51/149], Loss: 0.1736\n",
      "Epoch [9/10], Step [52/149], Loss: 0.2642\n",
      "Epoch [9/10], Step [53/149], Loss: 0.2548\n",
      "Epoch [9/10], Step [54/149], Loss: 0.2358\n",
      "Epoch [9/10], Step [55/149], Loss: 0.2682\n",
      "Epoch [9/10], Step [56/149], Loss: 0.2798\n",
      "Epoch [9/10], Step [57/149], Loss: 0.2625\n",
      "Epoch [9/10], Step [58/149], Loss: 0.1886\n",
      "Epoch [9/10], Step [59/149], Loss: 0.2126\n",
      "Epoch [9/10], Step [60/149], Loss: 0.2052\n",
      "Epoch [9/10], Step [61/149], Loss: 0.3232\n",
      "Epoch [9/10], Step [62/149], Loss: 0.3127\n",
      "Epoch [9/10], Step [63/149], Loss: 0.2922\n",
      "Epoch [9/10], Step [64/149], Loss: 0.2639\n",
      "Epoch [9/10], Step [65/149], Loss: 0.1908\n",
      "Epoch [9/10], Step [66/149], Loss: 0.2369\n",
      "Epoch [9/10], Step [67/149], Loss: 0.2220\n",
      "Epoch [9/10], Step [68/149], Loss: 0.1886\n",
      "Epoch [9/10], Step [69/149], Loss: 0.3330\n",
      "Epoch [9/10], Step [70/149], Loss: 0.2218\n",
      "Epoch [9/10], Step [71/149], Loss: 0.2623\n",
      "Epoch [9/10], Step [72/149], Loss: 0.2552\n",
      "Epoch [9/10], Step [73/149], Loss: 0.1888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 55\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_total_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\_functional.py:110\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m--> 110\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = models.vgg16(pretrained=True)\n",
    "num_ftrs = model_ft.classifier[0].in_features\n",
    "print(num_ftrs)\n",
    "\n",
    "model_ft.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=num_ftrs, out_features=1024),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=1024, out_features=512),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=512, out_features=128),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(in_features=128, out_features=2)\n",
    ")\n",
    "\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.6)\n",
    "\n",
    "since = time.time()\n",
    "lowest_loss = []\n",
    "accuracy_list = []\n",
    "val_acc_list= []\n",
    "n_total_steps = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    lowest_in_epoch = math.inf\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_label = []\n",
    "    best_pred = []\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        labels = labels\n",
    "        # Forward pass\n",
    "        outputs = model_ft(images)\n",
    "#         print(labels)\n",
    "#         print(outputs)\n",
    "#         print(f'Shape of training data: {outputs.shape}')\n",
    "#         print(f'Shape of lables: {labels.shape}')\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "#         print(total)\n",
    "        correct += (pred.numpy() == labels.numpy()).sum()\n",
    "#         print(correct)\n",
    "        if loss.item() < lowest_in_epoch:\n",
    "            lowest_in_epoch = loss.item()\n",
    "            best_label = labels\n",
    "            best_pred = outputs\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] and the best match:')\n",
    "    print(best_label)\n",
    "    print(best_pred)\n",
    "    lowest_loss.append((epoch, lowest_in_epoch))\n",
    "    accuracy = 100 * correct // total\n",
    "    accuracy_list.append((epoch, accuracy))\n",
    "    scheduler.step()\n",
    "    \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for images, labels in validationloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_ft(images)\n",
    "        _, val_pred = torch.max(outputs.data, 1)\n",
    "        val_correct += (val_pred.numpy() == labels.numpy()).sum()\n",
    "        val_total += labels.size(0)\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_acc_list.append((epoch, val_accuracy))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b046e223",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(0, 0.6694808006286621),\n",
       "  (1, 0.6769370436668396),\n",
       "  (2, 0.6681615710258484),\n",
       "  (3, 0.6139336824417114),\n",
       "  (4, 0.5770681500434875),\n",
       "  (5, 0.4304210841655731),\n",
       "  (6, 0.32195156812667847),\n",
       "  (7, 0.259631484746933)],\n",
       " [(0, 51), (1, 52), (2, 53), (3, 57), (4, 63), (5, 72), (6, 78), (7, 84)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_loss, accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "616054a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.41290594685787\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in testloader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model_ft(images)\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "#         print(total)\n",
    "    correct += (pred.numpy() == labels.numpy()).sum()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16e541a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "for i in range(8):\n",
    "    x_list.append(i)\n",
    "    y_list.append(lowest_loss[i][1])\n",
    "plt.plot(x_list, y_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show\n",
    "plt.savefig(\"genderclassification-loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c23f79a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_list = []\n",
    "y_list = []\n",
    "for i in range(8):\n",
    "    x_list.append(i)\n",
    "    y_list.append(accuracy_list[i][1])\n",
    "plt.plot(x_list, y_list)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show\n",
    "plt.savefig(\"genderclassification-accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc79b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
